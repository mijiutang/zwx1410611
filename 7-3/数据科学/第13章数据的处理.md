## 第13章 数据的处理

### 13.1 Hadoop

大数据时代，需要解决大量数据、异构数据等多种问题带来的数据处理难题，这里将主要介绍Hadoop。
Hadoop是一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。Hadoop构建了一个分布式文件系统——Hadoop Distributed File System(HDFS)。HDFS有着高容错性的特点，而且它提供高传输率来访问应用程序的数据，适合那些有着超大数据集的应用程序。HDFS放宽了POSIX的要求，这样可以以流的形式访问文件系统中的数据。

#### 13.1.1 Hadoop的起源

##### 13.1.1.1 项目起源
Hadoop由Apache Software Foundation公司于2005年秋作为Lucene的子项目Nutch的一部分正式引入。它受到最先由Google Lab开发的MapReduce和Google File System(GFS)的启发。2006年3月，MapReduce和Nutch Distributed File System(NDFS)分别被纳入Hadoop项目中。
Hadoop是在互联网上对搜索关键字进行内容分类的最受欢迎的工具，而且它也可以解决许多具有极大伸缩性的问题。例如，如果要搜索一个10TB的巨型文件，会出现什么情况?在传统的系统上，这将需要很长的时间。但是Hadoop在设计时就考虑到这些问题，采用并行执行机制，因此能大大提高效率。

##### 13.1.1.2 名字起源
Hadoop这个名字不是一个缩写，它是一个虚构的名字。该项目的创建者——道格·卡廷解释Hadoop的得名：“这个名字是我的孩子给一个棕黄色的大象玩具起的名字。我的命名标准就是简短、容易发音和拼写，没有太多的意义，并且不会被用于别处。小孩子恰恰是这方面的高手。”

#### 13.1.2 优点
Hadoop是一个能够对大量数据进行分布式处理的软件框架。但是Hadoop是以一种可靠、高效、可伸缩的方式进行处理的。Hadoop是可靠的，因为它假设计算元素和存储会失败，因此它维护多个工作数据副本，确保能够针对失败的节点重新分布处理；Hadoop是高效的，因为它以并行的方式工作，通过并行处理加快处理速度；Hadoop还是可伸缩的，能够处理PB级数据。此外，Hadoop依赖于社区服务器，因此它的成本较低，任何人都可以使用。
Hadoop是一个能够使用户轻松架构和使用的分布式计算平台。用户可以轻松地在Hadoop上开发和运行处理海量数据的应用程序。它主要有以下几个优点：
1. **高可靠性**：Hadoop按位存储和处理数据的能力值得人们信赖。
2. **高扩展性**：Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。
3. **高效性**：Hadoop能在节点间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。
4. **高容错性**：Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。
Hadoop带有用Java语言编写的框架，因此运行在Linux生产平台上是非常理想的。Hadoop上的应用程序也可以使用其他语言编写，比如C++。

#### 13.1.3 架构
Hadoop由许多元素构成。其最底部是HDFS，它存储Hadoop集群中所有存储节点上的文件。HDFS(对于本书)的上一层是MapReduce引擎，该引擎由JobTrackers和TaskTrackers组成。

##### 13.1.3.1 HDFS
对外部客户机而言，HDFS就像一个传统的分级文件系统。可以创建、删除、移动或重命名文件等。但是HDFS的架构是基于一组特定的节点构建的，这是由它自身的特点所决定的。这些节点包括NameNode(仅一个)，它在HDFS内部提供元数据服务；DataNode，它为HDFS提供存储块。由于仅存在一个NameNode，因此这是HDFS的一个缺点(单点失败)。
存储在HDFS中的文件被分成块，然后将这些块复制到多个计算机中(DataNode)。这与传统的RAID架构大不相同。块的大小(通常为64MB)和复制的块数量在创建文件时由客户机决定。NameNode可以控制所有文件操作。HDFS内部的所有通信都基于标准的TCP/IP协议。

##### 13.1.3.2 NameNode
NameNode是一个通常在HDFS实例中的单独机器上运行的软件。它负责管理文件系统名称空间和控制外部客户机的访问。NameNode决定是否将文件映射到DataNode的复制块上。对于最常见的3个复制块，第1个复制块存储在同一机架的不同节点上，最后一个复制块存储在不同机架的某个节点上。
实际的I/O事务并没有经过NameNode，只有以块的形式出现的元数据经过NameNode。当外部客户机发送请求要求创建文件时，NameNode会以块标识和该块的第一个副本的DataNode IP地址作为响应。这个NameNode还会通知其他将要接收该块副本的DataNode。
NameNode在一个称为FsImage的文件中存储所有关于文件系统名称空间的信息。这个文件和一个包含所有事务的记录文件(这里是EditLog)将存储在NameNode的本地文件系统上。FsImage和EditLog文件也需要复制副本，以防文件损坏或NameNode系统丢失。
NameNode本身不可避免地具有单点失效(Single Point of Failure,SPOF)的风险，主备模式并不能解决这个问题，通过Hadoop Non-stop Namenode才能实现100%可用时间。

##### 13.1.3.3 DataNode
DataNode也是一个通常在HDFS实例中的单独机器上运行的软件。Hadoop集群包含一个NameNode和大量DataNode。DataNode通常以机架的形式组织，机架通过一个交换机将所有系统连接起来。Hadoop的一个假设是：机架内部节点之间的传输速度快于机架间节点的传输速度。
DataNode响应来自HDFS客户机的读写请求。它们还响应来自NameNode的创建、删除和复制块的命令。NameNode依赖来自每个DataNode的定期心跳消息。每条消息都包含一个块报告，NameNode可以根据这个报告验证块映射和其他文件系统元数据。如果DataNode不能发送心跳消息，NameNode将采取修复措施，重新复制在该节点上丢失的块。

##### 13.1.3.4 文件操作
HDFS并不是一个万能的文件系统。它的主要目的是支持以流的形式访问写入的大型文件。如果客户机想将文件写入HDFS，首先需要将该文件缓存到本地的临时存储。如果缓存的数据大于所需的HDFS块大小，创建文件的请求将发送给NameNode。NameNode将以DataNode标识和目标块响应客户机，同时也通知将要保存文件块副本的DataNode。当客户机开始将临时文件发送给第一个DataNode时，将立即通过管道方式将块内容转发给副本DataNode。客户机也负责创建保存在相同HDFS名称空间中的校验文件。在最后的文件块发送之后，NameNode将文件创建提交到它的持久化元数据存储。

##### 13.1.3.5 Linux集群
Hadoop框架可在单一的Linux平台上使用(开发和调试时)，但是使用存放在机架上的商业服务器才能发挥它的力量。这些机架组成一个Hadoop集群。它通过集群拓扑知识决定如何在整个集群中分配作业和文件。Hadoop假定节点可能失败，因此采用本机方法处理单个计算机甚至所有机架都有可能失败。

#### 13.1.4 MapReduce流程
Hadoop的最常见用法之一是Web搜索。虽然它不是唯一的软件框架应用程序，但作为一个并行数据处理引擎，它的表现非常突出。Hadoop最有趣的方面之一是Map and Reduce流程，它受到谷歌开发的启发。这个流程称为创建索引，它将Web爬行器检索到的文本Web页面作为输入，并且将这些页面上的单词的频率报告作为结果。然后可以在整个Web搜索过程中使用这个结果从已定义的搜索参数中识别内容。
最简单的MapReduce应用程序至少包含3个部分：1个Map函数、1个Reduce函数和1个main函数。main函数将作业控制和文件输入/输出结合起来。在这点上，Hadoop提供了大量的接口和抽象类数据，从而为Hadoop应用程序的开发人员提供许多工具，可用于调试和性能度量等。
MapReduce本身就是用于并行处理大数据集的软件框架。MapReduce的根源是函数性编程中的Map和Reduce函数。它由两个可能包含有许多实例(许多Map和Reduce)的操作组成。Map函数接受一组数据并将其转换为一个键/值对列表，输入域中的每个元素对应一个键/值对。Reduce函数接受Map函数生成的列表，然后根据它们的键(为每个键生成一个键/值对)缩小键—值对列表。
这里提供一个示例。假设输入域是“one small step for man,one giant leap for mankind”，那么在这个域上运行Map函数将得出以下的键/值对列表：
```
(one,1)(small,1)(step,1)(for,1)(man,1)(one,1)(giant,1)(leap,1)(for,1)(mankind,1)
```
如果对这个键/值对列表应用Reduce函数，将得到以下一组键/值对：
```
(one,2)(small,1)(step,1)(for,2)(man,1)(giant,1)(leap,1)(mankind,1)
```
结果是对输入域中的单词进行计数，这无疑对处理索引十分有用。但是，假设有两个输入域，第一个是one small step for man，第二个是one giant leap for mankind，可以在每个域上执行Map函数和Reduce函数，然后将这两个键/值对列表应用到另一个Reduce函数，这时得到与前面一样的结果。换句话说，可以在输入域并行使用相同的操作，得到的结果是一样的，但速度更快。这便是MapReduce的威力，它的并行功能可在任意数量的系统上使用。
回到Hadoop上，它是如何实现这个功能的?一个代表客户机在单个主系统上启动的MapReduce应用程序称为JobTracker。类似于NameNode，它是Hadoop集群中唯一负责控制MapReduce应用程序的系统。在应用程序提交之后，将提供包含在HDFS中的输入和输出目录。JobTracker使用文件块信息(物理量和位置)，确定如何创建其他TaskTracker从属任务。MapReduce应用程序被复制到每个出现输入文件块的节点，将为特定节点上的每个文件块创建一个唯一的从属任务。每个TaskTracker将状态和完成信息报告给JobTracker。
Hadoop的这个特点非常重要，因为它并没有将存储移动到某个位置以供处理，而是将处理移动到存储。

### 13.2 Spark

#### 13.2.1 概述
Spark发源于美国加州大学伯克利分校AMPLab的集群计算平台。2009年由Berkeley's AMPLab的马泰主导编写，于2010年开放源码，在2013年进入Apache孵化器项目，2014年成为Apache三个顶级项目之一。Spark被称为下一代计算平台。它立足于内存计算，从多迭代批量处理出发，兼容并蓄数据仓库、流处理和图计算等多种计算范式，是罕见的全能选手。
可以这样来描述Spark:基于内存计算的集群计算系统，设计目标是让数据分析更加快速，提供比Hadoop更上层的API，支持交互查询和迭代计算。

#### 13.2.2 Spark的特点
简言之，Spark的特点就是“轻”“快”“灵”“巧”。
所谓“轻”，是指Spark 0.6核心代码只有2万行，而Hadoop 1.0为9万行，2.0为22万行。一方面，要感谢Scala语言的简洁和丰富表达力；另一方面，Spark很好地利用了Hadoop和Mesos(伯克利另一个进入孵化器的项目，主攻集群的动态资源管理)的基础设施。虽然很轻，但Spark在容错设计上不打折扣。其主创人马泰声称：“不把错误当特例处理。”言下之意，容错是基础设施的一部分。
“快”是指Spark对小数据集能达到亚秒级的延迟，这对于Hadoop MapReduce是无法想象的(由于HDFS“心跳”间隔机制，仅任务启动就有数秒的延迟)。就大数据集而言，对典型的迭代机器学习、即席查询(ad-hoc query)、图计算等应用，Spark版本比基于MapReduce、Hive和Pregel的实现快十倍到百倍。。内存计算、数据本地性(locality)和传输优化、调度优化等该居首功，也与设计伊始即秉持的轻量理念不无关系。
“灵”是指Spark提供了不同层面的灵活性。在实现层，它完美演绎了Scala Trait动态混入(Mixin)策略(如可更换的集群调度器、序列化库);在原语(Primitive)层，它允许扩展新的数据算子(Operator)、新的数据源(如HDFS之外支持DynamoDB)、新的Language Bindings(Java和Python);在范式(Paradigm)层，Spark支持内存计算、多迭代批量处理、即席查询(ad-hoc query)、流处理和图计算等多种范式。
“灵”是指Spark提供了不同层面的灵活性。在实现层，它允许扩展新的数据算子(Operator)、新的数据源(如HDFS之外支持DynamoDB)在迭代批量处理、即席查询、流处理和图计算等多种范式。
“巧”是指Spark巧在借势和借力。它借助了Hadoop之势头，又与Hadoop无缝结合；Shark(Spark上的数据仓库实现)借了Hive的势；图计算借用Pregel和PowerGraph的API以及PowerGraph的点分割思想。一切都借助了Scala之势。

#### 13.2.3 编程模型
Spark的计算抽象是数据流，而且是带有工作集(Working set)的数据流。它的突破在于，在保证容错的前提下，用内存来承载工作集。显然，内存的存取速度快于磁盘多个数量级，从而可以极大地提升性能。
那么Spark如何实现容错?传统上有两种方法：日志和检查点。由于检查点方法有数据冗余和网络通信的开销，因此Spark 采用日志数据更新。由于Spark记录的是粗粒度的RDD更新，这样开销可以忽略不计。

Spark程序工作在两个空间中：Spark RDD空间和Scala原生数据空间。

在原生数据空间里，数据表现为标量(scalar, 即Scala基本类型，图13-1中用小方块表示)、集合类型(图13-1中用虚线框表示)和持久存储(图13-1中用圆柱表示)。

Spark编程模型中有多种算子：

1. **输入算子**：将Scala集合类型或存储中的数据吸入RDD空间，转为RDD。输入算子的输入大致有两类：
   - 一类针对Scala集合类型，如`Parallelize`；
   - 另一类针对存储数据。
   输入算子的输出就是Spark空间的RDD。

2. **变换算子**：RDD经过变换算子生成新的RDD。在Spark运行时，RDD会被划分成很多的分区(Partition)分布到集群的多个节点中。注意，分区是个逻辑概念，变换前后的新旧分区在物理上可能是同一块内存或存储。这是很重要的优化，以防止函数式不变性导致的内存需求无限扩张。
   一部分变换算子视RDD的元素为简单元素；另一部分变换算子针对Key-Value集合。

3. **缓存算子**：有些RDD是计算的中间结果，其分区并不一定有相应的内存或存储与之对应，如果需要(如以备未来使用)，可以调用缓存算子将分区物化(Materialize)存下来。

4. **行动算子**：行动算子的输入是RDD(以及该RDD在Lineage上依赖的所有RDD)，输出是执行后生成的原生数据，可能是Scala标量、集合类型的数据或存储。当一个算子的输出为上述类型时，该算子必然是行动算子，其效果则是从RDD空间返回原生数据空间。

要注意的是，从RDD到RDD的变换算子序列，一直在RDD空间发生。但计算并不实际发生，只是不断地记录到元数据。

元数据的结构是DAG(有向无环图)，其中每一个“顶点”是RDD(包括生产该RDD的算子)，从父RDD到子RDD有“边”，表示RDD间的依赖性。

Spark给元数据DAG取了个很酷的名字——世系(Lineage)。由世系来实现日志更新。世系一直增长，直到遇上行动算子，这时就要评估了，把刚才累积的所有算子一次性执行。

#### 13.2.4 运行和调度

Spark程序由客户端启动，分两个阶段(图13-2):

1. 第一阶段记录变换算子序列、增量构建DAG图；
2. 第二阶段由行动算子触发，DAG Scheduler把DAG图转化为作业及其任务集。

![图13-2 Spark程序运行示意图](图13-2)

在Spark中有一个问题非常重要，就是对依赖的描述。Spark将依赖划分为两类：窄依赖和宽依赖(图13-3)。

![图13-3 窄依赖和宽依赖](图13-3)

窄依赖是指父RDD的每一个分区最多被一个子RDD的分区所用。而宽依赖则是指子RDD的分区依赖于父RDD的所有分区，通常对应于shuffle类操作。

在Spark程序中，窄依赖对于优化很有用处。这是因为Spark的编程模型中，逻辑上，每个RDD的算子都是一个fork/join(指同步多个并行任务的barrier)——即把计算fork到每个分区，算完后join，然后fork/join下一个RDD的算子。

如果直接翻译到物理实现，是很不经济的。因为，每一个RDD(即使是中间结果)都需要物化到内存或存储中，费时费空间；而且join作为全局的barrier，是很昂贵的，会被最慢的那个节点拖死。

如果子RDD的分区到父RDD的分区是窄依赖，就可以实施经典的fusion优化，把两个fork/join合为一个；如果连续的变换算子序列都是窄依赖，就可以把很多个fork/join并为一个，不但减少了大量的全局barrier，而且无须物化很多中间结果RDD，这将极大地提升性能。Spark把这种方式叫作流水线优化。

但变换算子序列一碰上shuffle类操作，流水线优化就终止了。

在具体实现中，Spark的DAG调度器会从当前算子往前回溯依赖图，一碰到宽依赖，就生成一个Stage来容纳已遍历的算子序列。

在这个Stage里，可以安全地实施流水线优化。然后，又从那个宽依赖开始继续回溯，生成下一个Stage。

如图13-4中所示，DAG调度器从G开始回溯。F到G之间是宽依赖，因此，将F和G划分为不同的Stage。而B与G之间是窄依赖，所以继续回溯，直到找到A与B之间的宽依赖，从而在那里划分Stage。

宽/窄依赖的概念不止用在调度中，对容错也很有用。如果一个节点宕机了，而且运算是窄依赖，那只要把丢失的父RDD分区重算即可，与其他节点没有依赖。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。

所以如果使用检查点算子来做检查点，不仅要考虑世系是否足够长，也要考虑是否有宽依赖，对宽依赖加检查点是最物有所值的。
