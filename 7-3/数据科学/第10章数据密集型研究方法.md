## 第10章 数据密集型研究方法

### 1

#### 10.1 范式和范式的演化过程

##### 10.1.1 范式的定义

“范式”的英文为 “Paradigm”，意指“模范”或“模型”。

“范式”最初是由美国著名科学哲学家托马斯·库恩在《科学革命的结构》中提出的一个词语，此著一问世，这个词汇立刻被学界接受，并且围绕它展开了激烈的争论。库恩长期以来研究科学史，他发现一种累积性的科学史观统治着这个领域，但这种认识他认为是不能真正反映科学史原貌的。库恩自述，一旦他找到了“范式”这个词汇，一切困惑当即冰释。不过这个“范式”，却并不是一个简单的字眼。如果非要简单地概括，它的内涵有两层：①科学共同体的共同承诺集合。②科学共同体共有的范例。

范式概念是库恩范式理论的核心，而范式从本质上讲是一种理论体系。范式理论指常规科学所赖以运作的理论基础和实践规范。范式是从事某一科学研究的研究者群体所共同遵从的世界观和行为方式，它包括三个方面的内容：共同的基本理论、观念和方法；共同的信念；某种自然观(包括形而上学假定)。范式的基本原则可以在本体论、认识论和方法论三个层次表现出来，分别回答的是事物存在的真实性问题、知者与被知者之间的关系问题以及研究方法的理论体系问题。这些理论和原则对特定的科学家共同体起规范的作用，协调他们对世界的看法以及他们的行为方式。

库恩指出：“按既定的用法，范式就是一种公认的模型或模式。我采用这个术语是想说明，在科学实践活动中某些被公认的范例——包括定律、理论、应用以及仪器设备统统在内的范例——为某种科学研究传统的出现提供了模型。”在库恩看来，范式是一种对本体论、认识论和方法论的基本承诺，是科学家集团所共同接受的一组假说、理论、准则和方法的总和，这些东西在心理上形成科学家的共同信念。范式的特点是：

1. 范式在一定程度内具有公认性。
2. 范式是一个由基本定律、理论、应用以及相关的仪器设备等构成的一个整体，它的存在给科学家提供了一个研究纲领。
3. 范式还为科学研究提供了可模仿的成功先例。

可以看出，在库恩的范式理论里，范式归根到底是一种理论体系，范式的突破导致科学革命，从而使科学获得一个全新的面貌。库恩对范式的强调对促进心理学中的理论研究具有重要意义。

学术范式就是看待研究对象的方式和视角，它决定了我们如何看待对象，把对象看成什么，在对象中看到什么、忽视什么。

模式 (Pattern) 其实就是解决某一类问题的方法论。即把解决某类问题的方法总结归纳到理论高度，那就是模式。按既定的用法，范式就是一种公认的模型或模式。

亚历山大给出的经典定义是：每个模式都描述了一个在环境中不断出现的问题，然后描述了针对该问题的解决方案的核心。通过这种方式，可以无数次地使用那些已有的解决方案，无须重复相同的工作。

模式有不同的领域，建筑领域有建筑模式，软件设计领域也有设计模式。当一个领域逐渐成熟的时候，自然会出现很多模式。

##### 10.1.2 范式的演变过程

一个稳定的范式如果不能提供解决问题的适当方式，它就会变弱，从而出现范式转移 (Paradigm Shift)。按照库恩的定义，范式转移就是新的概念传统，是解释中的激进改变，科学据此对某一知识和活动领域采取全新的和变化了的视角。通常，范式转移是一个由某一特别事件引发的过程。所谓特别事件是指在现有范式中被证明是反常 (Anomalous) 事件的增加，为了纠正问题，决策者需要改变工具设定，并尝试新的政策工具。如果这些努力不能奏效，就会出现政策失败 (Policy Failure)，进而打击旧的范式，促使人们去寻找新的范式，进行修正政策的试验过程。

库恩对范式转换对科学发展的推动作用尤其重视，他甚至认为，科学的发展不是靠知识的积累而是靠范式的转换完成的，一旦形成了新范式，就可以说建立起了常规科学。

库恩认为科学的发展可以看作一个循环过程：前学科(没有范式)→常规科学(建立范式)→科学革命(范式动摇)→新常规科学(建立新范式)。

在前学科时期，科学家之间存在意见分歧，因而没有一个被共同接受的范式。不同范式之间竞争和选择的结果是一种范式得到大多数科学家的支持，形成科学共同体公认的范式，于是进入常规科学时期。在此期间，科学共同体的主要任务是在范式的指导下从事释疑活动，通过释疑活动推动科学的发展。而随着在释疑活动过程中，一些新问题和新事物逐渐产生，并动摇了原有的范式，于是进入科学革命时期。革命的结果是拥有新范式的新的科学共同体取代拥有旧范式的旧的科学共同体。新范式的产生并不表示新范式更趋近真理，只是解题能力的增强。于是，再次进入新常规科学时期。

近半个世纪以来，科学范式理论对世界学术界产生了重大和深远的影响，很多学者都关注科学研究的范式，各个学科也纷纷开展自己的学科范式以及范式的应用研究。科学范式的价值不仅在于它描述了科学研究已有的习惯、传统和模式，还在于它提供了科学研究群体协同一致、共同探索的纽带，它能够为科学研究的未来发展和进一步开拓奠定基础。

###### 10.1.2.1 经验范式

经验范式是指偏重于经验事实的描述和明确具体的实用性的科学研究范式。经验范式在研究方法上，以归纳为主，带有较多盲目性的观测和实验。一般科学的早期阶段都属于经验科学，化学尤甚。在恩格斯《自然辩证法》中，经验范式专指18世纪以前搜集材料阶段的科学。

经验范式研究的经典方法是“三表法”:先观察，进而假设，再根据假设进行实验。如果实验的结果与假设不符合，则修正假设再实验。因此经验范式的模型是科学实验。

经验范式的经典范例如伽利略在比萨斜塔所做的“两个铁球同时落地”的著名实验。

牛顿的经典力学、哈维的血液循环学说以及后来的热力学、电学、化学、生物学、地质学等都是实验科学的典范。

###### 10.1.2.2 理论范式

在说明什么是理论范式之前，首先来看什么是理论。

所谓理论，是指人类对自然、社会现象按照已有的实证知识、经验、事实、法则、认知以及经过验证的假说，经由一般化与演绎推理等方法，进行合乎逻辑的推论性总结。人类借由观察实际存在的现象或逻辑推论，而得到某种学说，但如果未经社会实践或科学试验证明，只能属于假说。而如果假说能借由大量可重现的观察与实验而验证，并为众多科学家认定，这项假说就可被称为理论。

因此，理论范式主要指偏重理论总结和理性概括，强调较高普遍的理论认识而非直接实用意义的科学研究范式。

在研究方法上，理论范式以演绎法为主，不局限于描述经验事实。

在恩格斯《自然辩证法》中，理论范式主要指19世纪以后成熟起来的，处于整理材料阶段的科学。

理论范式的模型为数学模型。理论范式研究的范例包括：数学中的集合论、图论、数论和概率论；物理学中的相对论、弦理论、卡鲁扎一克莱恩理论(KK 理论)、圈量子引力理论；地理学中的大陆漂移学说、板块构造学说；气象学中的全球暖化理论；经济学中的微观经济学、宏观经济学以及博弈论；计算机科学中的算法信息论、计算机理论等。

###### 10.1.2.3 模拟范式

模拟范式是一个与数据模型构建、定量分析方法以及利用计算机来分析和解决科学问题的科学研究范式。主要用于对各个科学学科中的问题进行计算机模拟和其他形式的计算。

模拟范式研究的问题域包括：

1. 数值模拟。数值模拟有各种不同的目的，取决于被模拟的任务的特性。重建和理解已知事件(如地震、海啸和其他自然灾害);预测未来或未被观测到的情况(如天气、亚原子粒子的行为)。
2. 模型拟合与数据分析。适当调整模型或利用观察来解方程，不过也需要服从模型的约束条件(如石油勘探地球物理学、计算语言学);利用图论建立网络的模型，特别是那些相互联系的个人、组织和网站的模型。
3. 计算优化。包括数学优化；最优化已知方案(如工艺和制造过程、前端工程学)等。

模拟范式在研究中所用到的模型主要是计算机仿真/模拟。而典型的范例包括人工智能、热力学和分子问题、信号系统等。

###### 10.1.2.4 数据密集型研究范式

2007年，计算机图灵奖得主吉姆·格雷在美国国家研究理事会计算机科学和远程通信委员会 (NRC-CSTB) 的演讲报告中提出了科学研究“第四范式”，即以数据密集型计算为基础的科学研究范式。

格雷先生的四个科学范式理论基本内容为：第一范式产生于几千年前，是描述自然现象的，是以观察和实验为依据的研究，可称为经验范式：第二范式产生于几百年前，是以建模和归纳为基础的理论学科和分析范式，可称为理论范式；第三范式产生于几十年前，是以模拟复杂现象为基础的计算科学范式，可称为模拟范式；第四范式正在出现，是以数据考察为基础，联合理论、试验和模拟一体的数据密集计算范式，数据被一起捕获或由模拟器生成，被软件处理，信息和知识存储在计算机中，科学家使用数据管理和统计学方法分析数据库和文档，可称为数据密集型范式。

关于学科的发展，格雷认为，所有学科X都有两个进化分支，一个是模拟的X学，另一个是X一信息学，以生态学为例，即计算生态学和生态信息学，前者与模拟生态的研究有关，后者与收集和分析生态信息有关。在X一信息学中，编码和表达知识的方式是，将试验和设备产生的、其他档案产生的、文献中产生的、模拟产生的事实都保存在一个空间中，人们通过计算机向这个空间提问并获得答案，这之中要解决的一般问题有：数据的获取、PB级大容量数据的管理、公共模式的设置、数据的组织、数据的重组、数据的分享、查找和可视化工 具的构件、建立和实施模型的设置、数据和文献集成方法的构件、数据管理和长期保存的实现。当前，科学家们需要更好的工具来实现数据的捕获、分类管理、分析和可视化。

数据密集型研究范式是针对数据密集型科学，由传统的假设驱动向基于科学数据进行探索的科学方法的转变而生成的科学研究范式。

数据依靠工具获取或模拟产生；利用计算机软件处理；依靠计算机存储；利用数据管理和统计工具分析数据。

数据密集型研究范式的研究对象是科学数据。

#### 10.2 第四范式兴起的社会根源

##### 10.2.1 数据洪流的到来

从技术角度说，新型的硬件与数据中心、分布式计算、云计算、大容量数据存储与处理技术、社会化网络、移动终端设备、多样化的数据采集方式使海量数据的产生和记录成为可能。

从用户角度说，日益人性化的用户界面、人人的信息行为模式都容易作为数据记录下来，人人都可成为数据的提供方，人人也可成为数据的使用方。

从未来趋势看，随着云计算的发展，从理论上讲，世界上每个人、每件事存在和活动所产生的新数据，包括位置、状态、思考和行动等都能够被数字化，都能够成为数据在互联网传播。社交网站记录人们之间的互动和沟通，搜索引擎记录人们的搜索行为和搜索结果，电子商务网站记录人们购买商品的喜好，微博网站记录人们所产生的即时的想法和意见，图片视频分享网站记录人们的视觉观察，百科全书网站记录人们对抽象概念的认识，幻灯片分享网站记录人们的各种正式和非正式的演讲发言，机构知识库和开放获取期刊记录人们的学术研究成果。

上述现象导致了海量数据的产生，进而引发了数据洪流。可见，在现代技术的支持下，无论是简单的生活活动，还是复杂的学术研究，都能够成为数据被传播，这些海量数据蕴涵了巨大的潜力。善于挖掘、分析和可视化展现它们，将给人类的生活、工作和学习带来全方位的影响。

##### 10.2.2 科学界对海量数据的关注

2011年5月，麦肯锡全球研究院发布了一份同样关注当前社会数据洪流的报告——《海量数据：创新、竞争和生产率的下一个前沿》。报告以数字、数据和文档的当前状况为基础，分析大数据集如何在现代社会中创造价值和产生更大的潜力。报告称，2010年全球企业在磁盘上存储了超过7EB的新数据，消费者在个人计算机等设备上存储了超过6 EB的新数据，而1 EB等于10亿GB，相当于美国国会图书馆中存储数据的4000多倍。如果这些数据能够被合理地采集、管理和分析，将会创造难以计量的商业价值。报告通过研究美国卫生保健、欧洲公共部门、美国零售业、美国制造业和全球个人位置数据这五个领域的大数据集，总结出美国的医疗行业可以利用海量数据管理，通过使数据更易于访问、促进与数据相关的试验和商业决策自动化等手段，创造高达每年3000亿美元的价值；零售业通过海量数据管理可将利润率提高60%;欧盟可以利用海量数据管理缩减1490亿美元的运营开支。

在科学领域，由于科学观察、试验和研究设备的进化、计算机辅助技术的发展以及大规模合作的科学态势，科学数据呈海量增长。据统计，大型天文观察望远镜投入运行后第一年，生产的数据就达到1.28 PB( 1×10¹⁵ Bytes)；欧洲分子生物实验室核酸序列数据库EMBL-Bank收到数据的速度每年递增200%；预算达30亿元的人类基因组计划 (Human Genome Project, HGP) 要揭开组成人体的4万个基因的30亿个碱基对的秘密，2008年产生1万亿碱基对的数据，2009年速率又翻一番。

科学界对海量数据对科学研究的影响已经开始重点关注，2011 年2月美国《科学》 (Science) 期刊刊登了一个专辑，名为“数据处理 (Dealing With Data)”。该杂志还联合美国科学促进会 (AAAS) 的官方刊物《科学——信号传导》(Science:Signaling)、《科学——转化医学》(Science:Translational Medicine) 以及职业在线网站 Science Careers, 推出相关专题，围绕科学研究海量数据的问题展开讨论。

2006年美国国家科学基金会发布的名为《21世纪发现的赛博基础结构》的报告称，美国在科学和工程领域的领先地位将越来越取决于利用数字化科学数据，借助复杂的数据挖掘、集成、分析和可视化工具，将数据转换为信息和知识的能力。2010年12月，美国总统科技顾问委员会(PCAST) 提交给总统和国会的报告中明确提出“数据密集的科学和工程”(DISE) 概念，随后，在美国国家科学局和国家科学基金会的一些会议上深入地讨论了数据密集的科学和工程问题。

学者们将科学研究型数据的来源归结为四类： 一是来自于测量仪器、传感设备记录仪器的观测型数据，如天文望远镜观测的数据；二是来自于物理学、医学、生物学、心理学等各学科领域的大型试验设备的试验型数据，如粒子加速器试验数据；三是来自于大规模模拟计算的计算型数据；四是来自于跨学科、横向研究的参考型数据，如人类基因数据。这些数据有些由于观测和试验的不可重复性，有些由于时间、设备和经济等其他条件的限制，数据获取难度大，因此长期有效保存数据、科学的管理数据、有条件共享数据和促进数据利用是极有意义和价值的一项工作。

科学界需要为应对数据洪流采取措施，需要从海量的数据中寻找科学的规律，需要考察数据密集性科学研究的未来。

##### 10.2.3 关联数据运动

互联网之父伯纳斯·李从对网络发展和演变的分析中同样也发现了数据在未来网络中的价值。2006年，他在讨论关于语义网项目的一份设计记录中提出了发展数据网络 (Web of Data) 的设想，并创造了“关联数据 (Linked Data)” 一词，提出数据网络的核心即关联数据。 2009年，他在TED 大会(即技术娱乐和设计大会，1984年由理查德·沃尔曼先生发起，每年3月在美国召集科学、设计、文学、音乐等领域的杰出人物，探索关于技术、社会和人的问题)上再次阐明了关联数据及其对数据网络的影响。关联数据就是用主体、谓词、客体三元组来表示资源的RDF(Resource Description Framework) 格式数据，关联数据描述了一种出版结构化数据让其能够互联和更加有用的方法，它依赖标准互联网技术，如HTTP 和 URIs，不是使用它们服务于人类可读的网页，而是扩展到以能被计算机自动阅读的方式分享信息。关联数据有别于互联网上的文件互联，它强调的是数据互联，将以前没有关联的数据链接到一起，允许用户发现、描述、挖掘、关联和利用数据(图10-1)。

![图10-1 关联数据运动](图10-1.png)

关联数据方法提出后受到社会的广泛响应，一些国际组织如W3C 、世界银行，社会公益机构如美国国会图书馆，大众媒体如 BBC 、纽约时报等纷纷加入关联数据出版发布的行列。 2007年5月，W3C 启动LOD(Linked Open Data) 项目，号召人们将数据按照关联数据要求发布，将数据源互联。至2010年9月，仅用了三年时间，已有很多数据提供者和网络开发者将数据发布过来，形成了具有203个数据集、包含250亿条的RDF 语句、拥有3.95亿个链接的巨大关联数据网络。

从以下欧洲委员会在关联数据所提供的支持和举措，便可以感受到关联数据的影响力：

欧洲委员会提供资金作为第七框架计划的一部分，支持出版和使用链接的开放数据，目的是改善一个全天候的基础结构，以监测使用情况，并改善数据质量，为数据出版者和消费者提供低的接入门槛，开发一个开放源数据处理工具图书馆，为处理链接数据与欧盟数据的联合而管理一个试验平台，支持社区教育和最佳实践。

欧洲委员会资助了杰出网络项目——行星数据项目 (The Planet Data Project), 致力于将欧洲在大规模数据管理方面的研究者聚合起来，这些数据包括遵从链接数据原则出版的语义网 RDF 数据。该项目的独特之处在于能够在项目进行过程中开放引进其他研究者提供的行星数据。

欧洲委员会投资650万欧元的资金支持LOD2 项目，以持续开展链接开放数据项目，该项目2010年9月开始，将持续到2014年完成。项目的目标是从“相互关联的数据中创造知识”，具体任务包括五个方面：开发可供企业使用的、在互联网上公开和管理大量结构化信息的工具和方法；开发来源于维基百科和 OpenStreetMap 的高质量、多领域、多语种的本体的试验平台和网络；开发基于机器自动从互联网中学习和从网络融合数据的算法；开发能够可靠跟踪来源、确保隐私和数据安全、评价信息质量的标准和方法；开发适宜的工具以搜索、浏览和创作链接数据。

##### 10.2.4 政府数据开放运动

由于新型网络技术在电子政府发展过程中的逐步应用，现在的互联网已不仅是政府提供信息和服务的平台，而且是公众与政府互动的、共同创造的平台，这种状态改变了政府与公众以及公众之间建立关联的方式，同时也逐步改变了电子政府信息管理和服务的方式。新时代的电子政府不再只满足于从提供的角度给公众更好的服务，而是提倡政府作为一个整体的、开放的平台为企业和公众开放更多的信息和数据，促进更多的创新应用，这就是Tim O'Reily 提出政府2.0时重点强调的观点。

政府信息资源占社会信息资源的绝大多数，政府所掌握的数据也同样可观，如果关联数据标准用于政府数据的开放中，必将为全球的数据空间贡献更多的数据容量。对于政府而言，政府数据的开放，意味着电子政府的发展进入一个全新的开放、透明、互动的电子政府新阶段，它使政府能够提供一个中心平台或门户，更好地满足决策制定者、科学研究者、企业和普通公众对政府信息资源的需求。开放政府数据的价值在于：

1. 可以使公众免费、便捷地获得政府的数据，促进政府信息透明。
2. 可以使公民更多、更好地参与政府决策，促进政府决策的民主化。
3. 可以获得更有效的公众反馈，增加公众与政府的协作性。
4. 可以促进公共数据的广泛应用，激发创新，促进政府信息资源的深度开发与重用，更快实现资源的价值。

自2009年以来，世界电子政府先进国家兴起了一股“数据民主化”浪潮，各国积极开展政府数据开放工作。美国政府承诺除了涉及国家安全和隐私之外的政府数据全部向公众开放。 2009年5月，美国政府将以前政府专有的数据库发布到网上，建立了全球第一个独立的政府数据门户www.data.gov，该举措标志着全世界政府数据开放运动的开端。伯纳斯·李也是政府数据上网的积极倡导者，他不仅通过 TED 会议的号召让公众可以访问和利用政府数据，通过真实的案例说明政府开放数据的价值，还在2010年1月亲自为英国政府数据网站揭幕。

2009年以来，政府数据开放发展迅速，成效显著。以美国政府数据网站为例，2009年5 月美国政府数据网站上线时，只有11个政府机构提供了76项数据集。现在，该网站不仅提供计算机可读和可处理的数据集，还提供了多种数据分析、过滤和管理的工具；不仅有政府提供数据的各种应用程序，还鼓励公众贡献数据的应用程序；不仅提供互联网上的应用，还提供移动终端的综合应用。2012年1月，该站点提供了390178个原始数据和地理空间数据集，1150个政府应用程序，236个政府开发的应用，85个移动终端应用。美国政府有31个州、13个城市、172个机构和子机构建立了数据网站，而与此同时，国际上也有28个国家、地区或国际组织开办了数据网站。

政府数据开放运动的价值不仅在于它提供了计算机可以直接处理的数据，还在于它提供了各种各样的作为基础设施的数据工具，包括结构间协作的数据工具、数据反馈工具、数据查找工具等。毫无疑问，从科学研究发展的角度看，全球正在兴起的政府数据开放运动，为基于数据科学研究基础架构的建立提供了良好的条件。

#### 10.3 对第四范式的分析

科学研究第四范式描绘了科学研究在当前的水平下科学发展新的增长点，《第四范式》一书通过多角度的分析展现了新的科研范式的现状、价值和意义。结合上述数据洪流产生的社会背景分析，数据储存、数据互联和数据挖掘的价值将是难以估量的。本部分在《第四范式》一书的基础上，分析数据密集型科学研究以及格雷第四范式的意义和价值。

##### 10.3.1 科学数据与科学研究的问题

科学界目前对科学研究范式和海量数据问题的探讨，让人们感受到了当前科学研究中存在的问题，这些问题可以分为两方面： 一是数据方面；二是科学研究方面。

1. **数据方面的问题**。
   (1) 缺少合理的数据保存、共享和重用制度保障。 一直以来，绝大多数科学数据作为科学研究的附属材料没有得到很好的处置，它们在个人笔记中或使用磁介质存储，随着时间的流逝，渐渐变得不具备可读性而最终被丢弃。
   (2) 数据爆炸。在21世纪，大量新科学数据被新的仪器全天候获取，同时信息在计算机模型的人工世界中生成，这使人们身处数据洪流中。
   (3) 缺乏有效的数据工具。虽然数据在急剧增多，人类存储数据和传输数据的能力在不断增强，但数据往往保存在分散的数据库中，目前科学研究领域对进行数据管理、分类、分析、挖掘的工具依旧缺乏。

2. **科学交流方面的问题**。

在科学研究的整个链条中，人们只得到了作为论文或研究结果出版后最高端的一部分成果，大量的数据为了文献发表栏目的需要被缩减到极小一部分。尽管科学界有少量利用数据或重用数据成功的科学研究范例，但是总体来看，已经存在的科学交流模式，未能发掘数据这个原始科学研究素材的价值和功能。在面临着数据洪流，面临着数据揭示更多元、更深刻、更全面的事物规律的可能性之时，科学交流体系的完善应该提上议事日程。

##### 10.3.2 解决方案

从《第四范式》一书以及从其他学者的相关研究中，从上述关联数据实验项目、政府数据开放的实施以及科学界海量数据管理和挖掘的实践探索中看到，解决上述问题的数据密集型科学研究范式正在出现，学者们所提出的解决方案主要体现在：

1. 建立整个学科研究资源完整的采集、存储、管理、分析、发布链条，这个链条中不仅有文献还要有数据存在，不仅有原始数据还要有派生数据存在，不仅有结果还要看到过程的存在。
2. 建立试验室数据管理系统，并形成长期的数据存档和追根溯源的机制。
3. 建立对所捕获数据的挖掘和分析的专门机构。
4. 大力开发数据捕获、分类管理和分析挖掘的新算法和新工具。
5. 开发新型文献及数据出版和发布的工具，开发新的出版模式，支持出版物的快速变革。
6. 建立支持数据交流、发布、随处响应的基础设施，其中包括计算机资源硬件、数据中心和高速网络、软件工具和中间件；建立互操作标准，支持数据之间，以及数据和信息之间的整合、获取、推断、思考和说明，支持国际分享数据和多个学科的紧密合作。
7. 建立融数据和文献于一体的新型数字图书馆，形成数据与信息融合的互操作架构。让科学研究的整个过程都可以在数字图书馆的电子环境中进行，并对所有人开放，使科学研究的素材、思路、过程和结论都能够得到传播和共享。
8. 制定国家政策，促进全科学链条信息和数据的接入和重用，提高科学研究者的生产率，加速科学研究的创新和发现速度。
9. 培育数据科学家，展开对数据的高质量管理和分析。

根据库恩的范式理论，科学向数据密集型科学研究范式转换的成功将标志着常规科学的形成，必将引发科学研究观念和研究方法的新突破和新发展。这个前景反映了未来科学的行为方式，若要保证科学的快速发展、保持科学研究的领先地位。

#### 10.4 数据科学研究的一般流程

试想一下，如果24小时站在窗前，以分钟为单位，计数并记录下走过窗边的人的数目； 或者从今天起，记录下一年内周围0.5千米内居住的人们每天发送的电子邮件数目；或者去当地医院，在血液样本中搜寻 DNA 的样式。也许会觉得这些事情让人毛骨悚然，但是事情本该如此，因为人们所在的世界，就是一台巨大的数据产生机器：

1. 当人们早上乘坐各种交通工具前往工作岗位时，会产生数据。
2. 当血液在身体内流淌时，会产生数据。
3. 当购物、发送电子邮件、浏览网页、参与股票市场时，会产生数据。
4. 当修建楼房时，当用餐时，当和朋友们聊天时，会产生数据。
5. 当工厂生产商品时，会产生数据。

也就是说，人们的生活历程，或者说世界的运行历程，就是一个数据产生的过程。人们之所以要研究数据，也就是因为人们想更好地了解世界，或者说，只有人们更好地理解这个数据产生的过程，才能帮助我们找到很多问题的答案。

数据，其实就代表了这个## 现实世界历程的某种轨迹，所以，研究数据的过程就是解析和了解世界运行历程的过程。图10-2所示为数据科学的研究流程。

![图10-2 数据科学的研究流程](图10-2.png)

所以，对数据科学的研究首先要从世界入手。

首先，现实世界中，有各色各样的人们在进行各种各样的行为。有人在浏览网页，有人在参与体育活动，有人在发送电子邮件。这些都会产生大量数据。那么，人们就要收集这些数据。

一开始收集到的是原始的数据，比如网页浏览日志文件、体育活动记录、电子邮件原件等。

有了这些原始数据，为了能够开展后续的分析，要对它们进行清理或预处理。因而，需要建立一整套的数据预处理流程。当原始数据经过了预处理过程后，就得到了规整而干净的数据。

在此基础上，将会开展探索性数据分析 (Exploratory Data Analysis)。也许，通过探索性数据分析，人们意识到手头的数据还不是干净的数据，那么需要再次进行预处理。或者，人们意识到数据还不够充分，那就需要重新收集数据。

完成了探索性数据分析后，人们可以运用诸如k-NN、线性回归、贝叶斯算法等，来建立数据模型。根据需要解决的是哪种类型的问题，人们确定需要选取的数据模型种类。

完成数据建模后，可以开始解读数据，为数据实现可视化展示，生成数据报告，或与研究者们交流研究心得。这是数据科学研究的一种成果形式，即形成数据分析报告，或者是决策支持文件等。

数据科学研究的另一种成果形式，就是构建一种“数据产品”。比如搜索评级算法，或者推荐系统等。数据产品会产生更多的数据，会与现实世界进行互动，从而完成一个循环。这正是数据科学区别于统计学的一大特征，即研究会反过来影响数据产生的过程，并形成往复。

这和气象预测有显著的区别。对于气象预测，预测模型并无法影响输出。人们可以预测下个星期有雨，但除非人们具有非凡的神力，不然，下个星期是否下雨并不是由人们决定的。但是，如果人们建立的是一个推荐系统，推荐给读者一本书，原因是很多读者都喜欢它，那么在这个过程中，推荐系统就发挥了显著的作用，许多读者喜欢这本书，也许就是推荐造成的。也就是说，人们的工作不是预测未来，而是在改变未来。

即，模型不仅是在观测和预测所关注的现象，而且正在试图改变它。
