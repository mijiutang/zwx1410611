 

## 第11章 数据的获取和预处理

### 11.1 数据的获取

要研究数据科学，最重要的前提是要有数据。而在研究数据如何获取之前，先来看看该如何定义数据类型。
这里需要先区分两个概念：数据的类型和数据的语义。
数据的语义，是指该数据项在现实世界中的意义，比如，该数据项表示的是某公司的名称，或者某天，或者某个人的高度，等等。
而数据的类型，则表征的是在该类数据上可执行的操作类型。

#### 11.1.1 数据的类型

1996年时，马里兰大学教授本·施奈德曼提出可以把数据分成以下七类：一维数据(1-D)、二维数据(2-D)、三维数据(3-D)、多维数据(n-D)、时态数据(Temporal)、层次数据(Hierarchical)和网络数据(Network)。

在本书中，参考1946年史丹利·史密斯·史蒂文斯发表的On the Theory of Scales of Measurement一文中对数据类型的定义方式，将数据分为四种类型：

1. **分类型数据(Categorical data)**: 对于这种类型数据，只关注它们之间是否存在相等或者不等的情况，因此，对于这种类型数据，能运行的数学操作为求解=或≠。
2. **排序型数据(Ordinal data)**: 对于这类型数据，它们存在着排序关系，因而，可以运行的数据操作为=、≠、>、<。
3. **区间型数据(Interval data)**: 这种类型数据，属于定量性数据，它们可以看作一个个几何点，因此直接比较它们是没有任何意义的，只有比较它们两两之间的差别才有意义。对于它们，可运行的数据操作为=、≠、>、<、+、-。
4. **比值型数据(Ratio data)**: 这种类型数据主要是测量产生的结果，也属于定量性数据。对于它们来说，原点是固定的，可以看作一个个几何向量，因此，可运行的数据操作为=、≠、>、<、+、-、*、÷。

而区间型数据和比值型数据，通常可以统称为定量型数据(Quantitative Data)。

举例来说，分析图11-1中所示的订单数据。
首先来区分数据的语义和类型。在图11-1中，第一行条目，表示的是这些数据的语义。它们代表的是表中的这些数据在现实世界中的含义，比如这些数据代表的是订单号，还是订单日期，还是订单的优先级别。

| 订单号 | 订单日期   | 订单优先级别 | 产品包装 | 产品折扣 | 运输日期   |
| ------ | ---------- | ------------ | -------- | -------- | ---------- |
| 3      | 10/14/2006 | 5-低         | 大盒     | 0.8      | 10/21/2006 |
| 6      | 2/21/2008  | 4-未规定     | 小包装   | 0.72     | 2/22/2008  |
| 32     | 7/16/2007  | 2-高         | 小包装   | 0.65     | 7/17/2007  |
| 32     | 7/16/2007  | 2-高         | 包装袋   | 0.44     | 7/17/2007  |
| 32     | 7/16/2007  | 2-高         | 中等盒   | 0.56     | 7/17/2007  |
| 32     | 7/16/2007  | 2-高         | 包装袋   | 0.6      | 7/17/2007  |
| 32     | 7/16/2007  | 2-高         | 小盒     | 0.78     | 7/17/2007  |
| 35     | 10/23/2007 | 4-未规定     | 小盒     | 0.7      | 10/24/2007 |
| 35     | 10/23/2007 | 4-未规定     | 小包装   | 0.65     | 10/25/2007 |
| 36     | 11/3/2007  | 1-紧迫       | 小包装   | 0.44     | 11/3/2007  |
| 65     | 3/18/2007  | 1-紧迫       | 包装袋   | 0.65     | 3/19/2007  |
| 66     | 1/20/2005  | 5-低         | 包装袋   | 0.7      | 1/20/2005  |
| 69     | 6/4/2005   | 4-未规定     | 小盒     | 0.65     | 6/6/2005   |
| 69     | 6/4/2005   | 4-未规定     | 包装袋   | 0.44     | 6/6/2005   |
| 70     | 12/18/2006 | 5-低         | 包装袋   | 0.45     | 12/23/2006 |
| 70     | 12/18/2006 | 5-低         | 小盒     | 0.56     | 12/23/2006 |
| 96     | 4/17/2005  | 2-高         | 小包装   | 0.7      | 4/19/2005  |
| 97     | 1/29/2006  | 3-中等       | 小包装   | 0.55     | 1/30/2006  |

图11-1 订单

在这个关系型表中，每一行代表了一个元组，而每一列代表的是一个属性。
接下来来看看每一列属性所属的数据类型：

1. **订单号**：订单号数据代表的是每个订单的标号。在其上只能判断是否存在相等或者不等的情况，所以，订单号数据应该属于分类型数据。
2. **订单日期**：订单日期数据代表的是每个订单产生的日期。它是一种定量型数据。但不能直接将两个订单的日期来进行比较，这没有任何意义。可以做的是比较两个订单日期之间的差别，因而它属于区间型数据。
3. **订单优先级别**：订单优先级别数据代表的是每个订单的优先级别。可以比较它们之间是否相等，也可以依据优先级别的高低对它们进行排序，因此，它属于排序型数据。
4. **产品包装**：产品包装数据代表的是每个产品的包装方式。同理，可以比较它们之间是否相等，也可以依据所用的包装的大小对它们进行排序，因此，它属于排序型数据。
5. **产品折扣**：产品折扣数据代表的是每个产品所获得的折扣。显然，属于定量型数据。而且，可以直接将两个数据进行比较，因此，它属于比值型数据。
6. **运输日期**：运输日期代表的是每个产品运输的日期，显然，它和订单日期一样属于区间型数据。

在图11-2中，用粗体标出了分类型数据，用斜体表示排序性数据，剩下的为定量型数据(包括区间型和比值型)。从这张图中，可以看出，分类型数据和排序性数据实际上代表的是数据可能存在的维度，它们可以看作一种描述性的数据，它们之间是互相独立的。而定量型数据则是一种对数据的量度，是可以用来分析的数字，它们之间是有依赖关系的。

| 订单号 | 订单日期   | 订单优先级别 | 产品包装 | 产品折扣 | 运输日期   |
| ------ | ---------- | ------------ | -------- | -------- | ---------- |
| **3**  | 10/14/2006 | *5-低*       | 大盒     | 0.8      | 10/21/2006 |
| **6**  | 2/21/2008  | *4-未规定*   | 小包装   | 0.72     | 2/22/2008  |
| **32** | 7/16/2007  | *2-高*       | 小包装   | 0.65     | 7/17/2007  |
| **32** | 7/16/2007  | *2-高*       | 包装袋   | 0.44     | 7/17/2007  |
| **32** | 7/16/2007  | *2-高*       | 中等盒   | 0.56     | 7/17/2007  |
| **32** | 7/16/2007  | *2-高*       | 包装袋   | 0.6      | 7/17/2007  |
| **32** | 7/16/2007  | *2-高*       | 小盒     | 0.78     | 7/17/2007  |
| **35** | 10/23/2007 | *4-未规定*   | 小盒     | 0.7      | 10/24/2007 |
| **35** | 10/23/2007 | *4-未规定*   | 小包装   | 0.65     | 10/25/2007 |
| **36** | 11/3/2007  | *1-紧迫*     | 小包装   | 0.44     | 11/3/2007  |
| **65** | 3/18/2007  | *1-紧迫*     | 包装袋   | 0.65     | 3/19/2007  |
| **66** | 1/20/2005  | *5-低*       | 包装袋   | 0.7      | 1/20/2005  |
| **69** | 6/4/2005   | *4-未规定*   | 小盒     | 0.65     | 6/6/2005   |
| **69** | 6/4/2005   | *4-未规定*   | 包装袋   | 0.44     | 6/6/2005   |
| **70** | 12/18/2006 | *5-低*       | 包装袋   | 0.45     | 12/23/2006 |
| **70** | 12/18/2006 | *5-低*       | 小盒     | 0.56     | 12/23/2006 |
| **96** | 4/17/2005  | *2-高*       | 小包装   | 0.7      | 4/19/2005  |
| **97** | 1/29/2006  | *3-中等*     | 小包装   | 0.55     | 1/30/2006  |

图11-2 订单数据的数据类型区分

#### 11.1.2 网络爬虫技术

##### 11.1.2.1 概述

网络爬虫(又被称为网页蜘蛛，网络机器人)，是一种按照一定的规则，自动地抓取互联网信息的程序或脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或蠕虫。
随着网络的迅速发展，互联网成为大量信息的载体，如何有效地提取并利用这些信息成为一个巨大的挑战。而网络爬虫技术正是一种可以帮助人们快速高效地从互联网上获取数据的手段。
一个通用的网络爬虫的框架如图11-3所示。

![图11-3 通用的网络爬虫框架](图11-3.png)

互联网上的网络爬虫各式各样，但爬虫爬取网页的基本步骤大致相同：

1. 人工给定一个URL作为入口，从这里开始爬取。互联网的可视图呈蝴蝶形，网络爬虫一般从蝴蝶形左边的结构出发。门户网站中包含大量有价值的链接。
2. 用运行队列和完成队列来保存不同状态的链接。对于大型数据而言，内存中的队列是不够的，通常采用数据库模拟队列。用这种方法既可以进行海量的数据抓取，还可以实现断点续抓功能。
3. 线程从运行队列读取队首URL，如果存在，则继续执行，反之则停止爬取。
4. 每处理完一个URL，将其放入完成队列，防止重复访问。
5. 每次抓取网页之后分析其中的URL(URL采用字符串形式，功能类似指针)，将经过过滤的合法链接写入运行队列，等待提取。
6. 重复步骤3)、4)、5)。

因此，从网络爬虫的角度来看，可以将互联网的所有页面分为五个部分(图11-4):

1. 已下载未过期网页。
2. 已下载已过期网页：抓取到的网页实际上是互联网内容的一个镜像与备份，互联网是动态变化的，一部分互联网上的内容已经发生了变化，这时，这部分抓取到的网页就已经过期了。
3. 待下载网页：也就是待抓取URL队列中的那些页面。
4. 可知网页：还没有抓取下来，也没有在待抓取URL队列中，但是可以通过对已抓取页面或待抓取URL对应页面分析获取到的URL。
5. 不可知网页：爬虫无法直接抓取下载的网页。

![图11-4 互联网网页的划分](图11-4.png)

##### 11.1.2.2 抓取策略

在爬虫系统中，待抓取URL队列是很重要的一部分。待抓取URL队列中的URL以什么样的顺序排列也是一个很重要的问题，因为这涉及先抓取哪个页面，后抓取哪个页面。互联网广阔无边，为了最大限度利用有限的资源，需要进行资源配置，并运用某些策略使爬虫优先爬取重要性较高的网页。决定这些URL排列顺序## 的方法，就叫作抓取策略。下面以图11-5为例来重点介绍几种常见的抓取策略：

### 1. 深度优先遍历策略

深度优先搜索策略从起始网页开始，选择一个URL进入，分析这个网页中的URL，选择一个再进入。如此一个链接一个链接地抓取下去，直到处理完一条路线之后再处理下一条路线。深度优先策略设计较为简单。然而门户网站提供的链接往往最具价值，PageRank也很高，但每深入一层，网页价值和PageRank都会相应地有所下降。这暗示了重要网页通常距离种子较近，而过度深入抓取到的网页却价值很低。同时，这种策略抓取深度直接影响着抓取命中率以及抓取效率，抓取深度是该种策略的关键。而且深度优先在很多情况下会导致爬虫的陷入(Trapped)问题产生，因此，此种策略很少被使用。

![图11-5 网页拓扑结构示例](图11-5.png)

如果按照深度优先遍历策略来抓取图11-5所示的网页结构，则遍历的路径为：A—F—G、E—H—I、B、C、D。

### 2. 宽度优先遍历策略

宽度优先遍历策略或称广度优先搜索策略，是指在抓取过程中，在完成当前层次的搜索后，才进行下一层次的搜索。该算法的设计和实现相对简单。在目前为了覆盖尽可能多的网页，一般使用宽度优先搜索方法。宽度优先遍历策略的基本思路是，将新下载网页中发现的链接直接插入待抓取URL队列的末尾。也就是指网络爬虫会先抓取起始网页中链接的所有网页，然后再选择其中的一个链接网页，继续抓取在此网页中链接的所有网页。

如果按照宽度优先遍历策略来抓取图11-5中所示的网页结构，则遍历路径为：A—B—C—D—E—F、H、G、I。

### 3. 反向链接数策略

反向链接数是指一个网页被其他网页链接指向的数量。反向链接数表示的是一个网页的内容受到其他人的推荐程度。因此，很多时候搜索引擎的抓取系统会使用这个指标来评价网页的重要程度，从而决定不同网页抓取的先后顺序。

在真实的网络环境中，由于广告链接、作弊链接的存在，反向链接数不能完全等同于网页的重要程度。因此，搜索引擎往往考虑一些可靠的反向链接数。

### 4. Partial PageRank 策略

Partial PageRank算法借鉴了PageRank算法的思想：对于已经下载的网页，连同待抓取URL队列中的URL，形成网页集合，计算每个页面的PageRank值，计算完之后，将待抓取URL队列中的URL按照PageRank值的大小排列，并按照该顺序抓取页面。

如果每次抓取一个页面，就重新计算PageRank值，运算量会比较大。一种折中的方案是：每抓取K个页面后，重新计算一次PageRank值。但是这种情况还会有一个问题：对于在已经下载的页面中分析出的链接，也就是之前提到的未知网页那一部分，暂时是没有PageRank值的。为了解决这个问题，会给这些页面一个临时的PageRank值：将这个网页所有入链传递进来的PageRank值进行汇总，这样就形成了该未知页面的PageRank值，从而参与排序。

### 5. 大站优先策略

对于待抓取URL队列中的所有网页，根据所属的网站进行分类。对于待下载页面数多的网站，优先下载。这个策略也因此叫作大站优先策略。

##### 11.1.2.3 更新策略

互联网是实时变化的，具有很强的动态性。网页更新策略主要用来决定何时更新之前已经下载过的页面。常见的更新策略有以下三种：

### 1. 历史参考策略

顾名思义，根据页面以往的历史更新数据，预测该页面未来何时会发生变化。一般来说，是通过泊松过程进行建模预测。

### 2. 用户体验策略

尽管搜索引擎针对某个查询条件能够返回数量巨大的结果，但是用户往往只关注前几页结果。因此，抓取系统可以优先更新那些显示在查询结果前几页中的网页，而后再更新那些后面的网页。这种更新策略也需要用到历史信息。用户体验策略保留网页的多个历史版本，并且根据过去每次内容变化对搜索质量的影响，得出一个平均值，用这个值作为决定何时重新抓取的依据。

### 3. 聚类抽样策略

前面提到的两种更新策略都有一个前提：需要网页的历史信息。这样就存在两个问题：第一，如果为每个系统保存多个版本的历史信息，无疑增加了很大的系统负担；第二，如果新的网页完全没有历史信息，就无法确定更新策略。

这种策略认为，网页具有很多属性，对于类似属性的网页，可以认为其更新频率也是类似的。要计算某一个类别网页的更新频率，只需要对这一类网页抽样，以它们的更新周期作为整个类别的更新周期。聚类抽样策略如图11-6所示。

![图11-6 聚类抽样策略](图11-6.png)

##### 11.1.2.4 分布式抓取系统

一般来说，抓取系统需要面对的是整个互联网上数以亿计的网页。单个抓取程序不可能完成这样的任务。往往需要多个抓取程序一起来处理。一般来说，抓取系统往往是一个分布式的三层结构，如图11-7所示。

![图11-7 分布式抓取系统结构](图11-7.png)

最底层是分布在不同地理位置的数据中心，在每个数据中心里有若干台抓取服务器，而每台抓取服务器上可能部署了若干套爬虫程序。这就构成了一个基本的分布式抓取系统。

对于一个数据中心内的不同抓取服务器，协同工作的方式有几种：

#### 1. 主从式(Master-Slave)

主从式基本结构如图11-8所示。

![图11-8 主从式基本结构](图11-8.png)

对于主从式而言，有一台专门的Master服务器来维护待抓取URL队列，它负责每次将URL分发到不同的Slave服务器，而Slave服务器则负责实际的网页下载工作。Master服务器除了维护待抓取URL队列以及分发URL之外，还要负责调解各个Slave服务器的负载情况，以免某些Slave服务器过于清闲或劳累。

这种模式下，Master往往容易成为系统的瓶颈。

#### 2. 对等式(Peer to Peer)

对等式的基本结构如图11-9所示。

![图11-9 对等式基本结构](图11-9.png)

在这种模式下，所有的抓取服务器在分工上没有不同。每一台抓取服务器都可以从待抓取URL队列中获取URL，然后获取该URL的主域名的哈希值H，然后计算H mod m(其中m是服务器的数量，以上图为例，m为3)，计算得到的数就是处理该URL的主机编号。

例如，假设对于URL www.baidu.com，计算器哈希值H=8，m=3，则H mod m=2，因此由编号为2的服务器进行该链接的抓取。假设这时候是0号服务器拿到这个URL，那么它将把该URL转给服务器2，由服务器2进行抓取。

这种模式有一个问题，当有一台服务器死机或者添加新的服务器，那么所有URL的哈希值都要变化。也就是说，这种方式的扩展性不佳。针对这种情况，又有一种改进方案被提出来。这种改进的方案是指，用一致性哈希法来确定服务器分工。其基本结构如图11-10所示。

![图11-10 一致性哈希法确定服务器分工](图11-10.png)

运用一致性哈希法可以将URL的主域名进行哈希运算，映射为0～2^32的某个数。而将这个范围平均地分配给m台服务器，根据URL主域名哈希运算的值所处的范围判断由哪台服务器来进行抓取。

如果某一台服务器出现问题，那么本该由该服务器负责的网页则按照顺时针顺延，由下一台服务器进行抓取。这样的话，即使某台服务器出现问题，也不会影响其他工作。

##### 11.1.2.5 开源网络爬虫

1. **Heritrix**

Heritrix是一个爬虫框架，可以加入一些可互换的组件。Heritrix是用来获取完整精确的网站内容的爬虫，除文本内容之外，它还获取其他非文本内容(如图片等)，并对其进行处理，且不对网页内容进行修改。当重复爬取相同URL时，不会对先前网页进行替换。

Heritrix主要有以下4步：

1. 在预定的URL中选择一个并获取。
2. 分析，并将结果归档。
3. 选择已经发现的感兴趣的URL，加入运行队列。
4. 标记已经处理过的URL。

Heritrix利用广度优先策略来进行网页获取，其主要部件都具有高效性和可扩展性。然而Heritrix也有一定的局限性，如：

1. 只支持单线程爬虫，多爬虫之间不能合作。
2. 操作复杂，对有限的资源来说是一个问题。
3. 在硬件使系统失败时，其恢复能力较差等。

2. **Nutch**

Nutch深度遍历网站资源，将这些资源抓取到本地，使用的方法都是分析网站每一个有效的URL并向服务器端提交请求来获得相应结果，生成本地文件及相应的日志信息等。

Nutch与Heritrix有几点差异，即：

1. Nutch只获取并保存可索引的内容。
2. Nutch可以修剪内容，或者对内容格式进行转换。
3. Nutch保存内容的格式为数据库优化格式，便于以后索引；且对于重复的URL，要以新内容刷新替换旧的内容。
4. Nutch从命令行运行、控制。
5. Nutch的定制能力不够强(不过现在已经有了一定改进)。

3. **Larbin**

Larbin不同于以上两种网络爬虫，它只抓取网页，而不提供包括分析网页、将结果存储到数据库以及建立索引等服务。

Larbin的目的是对页面上的URL进行扩展性的抓取，为搜索引擎提供广泛的数据来源。虽然工作能力较为单一，但Larbin胜在其高度可配置性和良好的工作效率(一个简单的Larbin的爬虫可以每天获取500万的网页)，这也是Larbin最初的设计理念。

4. **Lucene**

Lucene是一个基于Java的全文信息检索工具包，它本身不是一个完整的全文索引应用程序，而是用来为各种应用程序提供索引和搜索功能。只要能把要索引的数据转化的文本格式，Lucene就能对该文档进行索引和搜索。

Lucene采用的是一种称为反向索引(Inverted Index)的方法。因此，在用户输入查询条件的时候，Lucene能非常快地得到搜索结果。

对文档建立好索引后，搜索引擎首先会对关键词进行解析，然后在建立好的索引上进行查找并返回和用户输入的关键词相关联的文档。

### 11.2 数据预处理的目的

数据预处理是数据处理的过程中非常重要的一步。这个过程，也许会是整个数据处理流程中最耗费时间的一环。过程也许会很枯燥和烦闷，但是绝对不可或缺。

《探索性数据挖掘与数据清理》的作者西奥多·约翰逊和塔玛拉帕拉尼·达苏曾经说过：“根据我们的经验，最终数据挖掘的价值有百分之八十都取决于探索式数据分析和数据清洗的效果。”

为什么需要进行数据的预处理呢?因为在原始数据中存在着各种各样的问题。

1. **杂乱性**

杂乱性是指系统中的数据缺乏统一的标准和定义。具体表现形式包括：①在不同的数据来源中的同义异名情况，例如为了标示客户，一些数据来源用cust_id来区分，另一些则用cust_number；②不同数据来源采用的度量标准可能不同，比如对于性别，一些采用“Male”或“Female”来区分，另一些则采用“男”和“女”来区分；③对同一属性定义的类型不同，以工资为例，一些数据来源可能定义为Int型，另一些则将工资定义为Double型。

2. **重复性**

重复性是指同一事物在数据库中存在两条或多条完全相同的记录。这种情况非常常见，如实际使用过程中出现的意义相同或者可以表示同一信息的多个属性。例如：年龄和出生日期。在一次数据挖掘中，若考察的是年龄段和消费特征的关系，那么，这两个属性便为冗余的。因为用年龄或出生日期都可以计算得到年龄段，且结果相同。当然，若在数据挖掘时，考察的是年龄段、出生月份对消费特征的影响，那这两个属性就表示的是不同的信息，不可视为重复属性。由此可见，重复性是一种相对的概念，需要根据实际的分析目标来予以判断。

3. **不完整性**

不完整性是指系统设计的不合理或者使用过程中的某些因素所造成的属性值缺失或者不确定。或者是某个元组中缺少了某样或者某几样属性，甚至是多个元组直接缺失。造成这种情况的原因，也许是在数据输入时，某些数据可能被误认为不重要而删除掉了。或是某些数据由于存在不一致性，结果被删除了。

4. **存在噪声**

存在噪声是指测量变量中的随机错误或偏离期望的孤立点值。噪声数据的来源众多，起因也各异。

总之，各种各样的疏忽或错误，会导致“脏”数据的存在，而数据预处理的目的是：对这些原始数据进行处理，为数据挖掘过程提供干净、准确、简洁的数据，减少数据处理量，提高数据挖掘的效率和准确性。可以说，没有高质量的数据就没有高质量的数据挖掘结果。

按照所处理的内容不同，可以将数据预处理的主要任务分为以下几类：

1. **数据清理**

数据清理即填写空缺值、平滑噪声数据、识别删除孤立点，以及解决## 数据中的不一致性问题。
2. **数据集成**

数据集成即通过操作集成多个来源不同的数据库、数据立方或文件。
3. **数据变换**

数据变换即对原始数据进行规范化和聚集操作。
4. **数据规约**

数据规约即通过操作得到数据集的压缩表示，所得到的压缩表示将会小得多，但可以在其上得到与原始数据相同或相近的数据挖掘结果。

### 11.3 数据清洗

数据清洗(图11-11)的主要任务就是对原始数据进行处理，将“脏”数据转化为“干净的”数据。其主要任务包括：

1. 填补空缺值。
2. 平滑噪声数据。
3. 纠正不一致数据。
4. 消除冗余数据。

下面详细介绍填补空缺值和平滑噪声数据。

![图11-11 数据清洗](图11-11.png)

#### 11.3.1 填补空缺值

原始数据并不总是完整的，在很多情况下，会出现数据库中，很多条记录的对应字段为空的情况。

引起空缺值的原因很多，例如：

1. 设备异常。
2. 与其他已有数据不一致而被删除。
3. 因为误解而没有被输入的数据。
4. 在输入时，有些数据因为得不到重视而没有被输入。
5. 对数据的改变没有进行日志记载。

填补空缺值的方法一般有以下几种：

1. **直接忽略存在属性缺失的元组**：这种方法一般是在缺少类标号时使用(主要是针对分类或描述)。但是，这种方法的有效性不好，尤其是当属性缺少值的比例很大时。
2. **人工方式来填写空缺值**：这种方法会耗费大量人力和时间，因而不适用于大数据集。
3. **自动填充空缺值**：一般可以使用全局变量、属性的平均值、与给定元组属于同一类的所有样本的平均值，或者由回归、判定树、基于推导的贝叶斯形式化方法等确定的其他可能值来自动填充。

要注意的是：自动填充的方法会使数据分布产生倾斜，导致数据分布过度集中于数据空间的某端，造成“头重脚轻”或者“比萨斜塔”等不均匀的分布特点。数据分布倾斜性将造成运算效率上的“瓶颈”和数据分析结果的“以偏概全”。而且，不管采用了何种方式来推断空缺值，填入的值都可能是不正确的。因为，我们毕竟不知道空缺处真实的值是多少，而是使用现有数据的信息来推测的。

#### 11.3.2 平滑噪声数据

噪声数据，是指原始数据中所存在的随机错误或偏差。引起噪声数据的原因有很多，比如：

1. 数据收集工具的问题。
2. 数据输入错误。
3. 数据传输错误。
4. 技术限制。
5. 命名规则的不一致。

##### 11.3.2.1 分箱法处理噪声数据

分箱法是指把待处理的数据按照一定的规则放进一些箱子中，考察每一个箱子中的数据，再采用某种方法来分别对各个箱子中的数据进行处理的办法。

而这里，所谓的箱子就是指按照属性值划分的某个子区间。如果一个属性值处于某个子区间范围内，就称把该属性值放进这个子区间代表的“箱子”里。

采用分箱法来平滑处理噪声数据，必须首先确定两个问题：

1. 如何分箱?
2. 如何对每个箱子中的数据进行平滑处理?

要注意的一点是，在实施分箱之前，必须首先对记录集按目标属性值的大小进行排序。

先来看第一个问题：如何分箱?

分箱的方法一般有三种：

1. **等深分箱法**：又称等频率分箱法。即按照对象的个数来划分。具体来说，就是将对象范围划分为每块包含大致相同数量样本的N块。每箱具有相同的记录数，而每箱记录数就称为箱的权重，也叫作箱子的深度。这种分箱方法便于数据缩放，缺点则是绝对属性管理比较困难(即通常无法等分)。
2. **等宽分箱法**：又称等距离分箱法。即按照对象的值来划分。具体来说，就是将对象范围划分为等间隔的N块。如果A和B是最低和最高的属性值，那么间隔宽度W的计算方式是：

\[ W = \frac{B - A}{N} \]

通常说来，等宽分箱法是最简单的划分方法，但在使用它时可能会出现不少例外情形，而且它不能很好地处理歪斜数据。划分之后的数据集在整个属性值的区间上呈平均分布，每个箱子的区间范围是一个常量。

3. **用户自定义区间法**：即根据用户需要自定义区间来划分的一种方式。

下面以一个具体的例子来说明三种方法的区别：

**例11.1** 假设客户收入属性income排序后的值(单位为元): 800, 1000, 1200, 1500, 1500, 1800, 2000, 2300, 2500, 2800, 3000, 3500, 4000, 4500, 4800, 5000。

下面分别尝试用三种分箱方法来进行划分：

**解答**

1. **等深分箱法**：假定需要划分为四个箱子。由于原始数据一共有16个数据项，因此，每个箱子的深度应该为4。因此，划分后的结果为：

- 箱1: 800, 1000, 1200, 1500
- 箱2: 1500, 1800, 2000, 2300
- 箱3: 2500, 2800, 3000, 3500
- 箱4: 4000, 4500, 4800, 5000

2. **等宽分箱法**：同样假定需要划分为四个箱子。检查所有数据可知，最低属性值为800，最高属性值为5000，因此每个箱子的间隔宽度W为：

\[ W = \frac{5000 - 800}{4} = 1050 \]

所以，分箱法的结果为：

- 箱1: 800, 1000, 1200, 1500, 1500, 1800
- 箱2: 2000, 2300, 2500, 2800
- 箱3: 3000, 3500
- 箱4: 4000, 4500, 4800, 5000

3. **用户自定义区间法**：假定根据用户自定义，按照如下方式划分——将客户收入划分为1000元以下、1000～2000、2000～3000、3000～4000和4000元以上几组。则划分结果为：

- 箱1: 800
- 箱2: 1000, 1200, 1500, 1500, 1800, 2000
- 箱3: 2300, 2500, 2800, 3000
- 箱4: 3500, 4000
- 箱5: 4500, 4800, 5000

确定了分箱方法后，来看第二个问题，即如何对每个箱子中的数据进行平滑处理。

可以有三种方式来对每个箱子中的数据进行平滑处理：

1. **按箱平均值平滑处理**：即对同一箱值中的数据求平均值，用平均值代替该箱子中的所有数据。
2. **按箱边界平滑处理**：对于箱中的每个数据，观察它与箱子两个边界值的差异，用差异较小的那个边界值替代该数据。
3. **按箱中值平滑处理**：取箱子的中值，用来替代箱子中的所有数据。

在例11.1中，获得等宽分箱法的划分结果如下：

- 箱1: 800, 1000, 1200, 1500, 1500, 1800
- 箱2: 2000, 2300, 2500, 2800
- 箱3: 3000, 3500
- 箱4: 4000, 4500, 4800, 5000

下面以此为例，来展示三种平滑方法。

1. **按箱平均值平滑处理**。

需要计算每个箱子中数据的平均值，然后用平均值来代替该箱子中的所有数据。

对于第一个箱子，我们计算出其数据的平均值为

\[ \frac{800 + 1000 + 1200 + 1500 + 1500 + 1800}{6} = 1300 \]

因此，用1300代替该箱子中的所有数值。

接下来再计算第二个箱子中的所有数据的平均值，再用该平均值代替第二个箱子中的所有数据。重复该过程，直到计算完毕。

因此，平滑结果为：

- 箱1: 1300, 1300, 1300, 1300, 1300, 1300
- 箱2: 2400, 2400, 2400, 2400
- 箱3: 3250, 3250
- 箱4: 4575, 4575, 4575, 4575

2. **按箱边界值平滑处理**。

首先考察箱子1中的数据。

箱子1有两个边界值800和1800，接下来考察箱子每个数据与这两个边界值之差，选择具有较小差值的那个边界值来代替该数据。

第一个数值是800，它本身就是边界值之一，因此不变。

第二个数值是1000。它与边界值一800的差是200，与边界值二1800的差是800，因此用边界值一800来代替它。

第三个数值是1200，它与边界值一800的差是400，与边界值二1800的差是600，因此用边界值一800来代替它。

以此类推，直到所有箱子中的数据计算完毕。于是得到以下的平滑结果：

- 箱1: 800, 800, 800, 1800, 1800, 1800
- 箱2: 2000, 2000, 2800, 2800
- 箱3: 3000, 3500
- 箱4: 4000, 4000, 5000, 5000

3. **按箱中值平滑处理**。

首先计算每个箱子中数据的中值，然后用该中值代替此箱中所有数据。这和按箱平均值平滑处理的处理方法类似，只是替代值不同。

对于第一个箱子中的数据，计算得到其中值为1350，因此，用1350替代第一个箱子中的所有数据。依此类推，最后得到平滑结果为：

- 箱1: 1350, 1350, 1350, 1350, 1350, 1350
- 箱2: 2400, 2400, 2400, 2400
- 箱3: 3250, 3250
- 箱4: 4650, 4650, 4650, 4650

**例11.2** 将下列排序后的价格分为3个组，排序后的需平滑价格为：4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34。

1. 按照等深分箱法划分。
2. 按照等宽分箱法划分。
3. 对等深分箱法的结果分别按照箱平均值和箱边界值做平滑处理。

**解答**

1. 该数据已经完成了排序，因此只需要对其进行分箱操作。

所需要划分的数据一共有12项，需要划分为3组，因此每组数据应为4个。所以划分结果为：

- 箱1: 4, 8, 9, 15

- 箱2: 21, 21, 24, 25

- 箱3: 26, 28, 29, 

- ## 34

  2. **按照等宽分箱法划分**：

  由于需要划分为3个箱，因此其间隔宽度为：

  \[ W = \frac{34 - 4}{3} = 10 \]

  所以划分结果为：

  - 箱1: 4, 8, 9
  - 箱2: 15, 21, 21, 24
  - 箱3: 25, 26, 28, 29, 34

  3. **对等深分箱法的结果分别按照箱平均值和箱边界值做平滑处理**：

  - **按箱平均值进行平滑处理**：

  计算每个箱子中数据的平均值，得到如下平滑结果：

  - 箱1: \( \frac{4 + 8 + 9 + 15}{4} = 9 \)，结果为：9, 9, 9, 9
  - 箱2: \( \frac{21 + 21 + 24 + 25}{4} = 22.75 \)，结果为：22.75, 22.75, 22.75, 22.75
  - 箱3: \( \frac{26 + 28 + 29 + 34}{4} = 29.25 \)，结果为：29.25, 29.25, 29.25, 29.25

  - **按箱边界值进行平滑处理**：

  对于每个箱子中的数据，比较它们与每个边界值之间的差异，选取差异小的替代该数值，得到平滑结果为：

  - 箱1: 4, 4, 4, 15
  - 箱2: 21, 21, 25, 25
  - 箱3: 26, 26, 26, 34

  ##### 11.3.2.2 其他平滑噪声数据的方法

  除分箱法外，还可以用聚类和回归法来平滑噪声数据。

  **聚类方法**将相似的值组织成群或类，那么落在群或类外的值就是孤立点，也就是噪声数据。

  如图11-12所示，图中共形成了三个聚类，“+”号用来表示聚类的质心。聚类的质心就是聚类的平均点。不在任何聚类中的点称为孤立点，就是要去掉的噪声数据。

  ![图11-12 聚类平滑噪声数据](图11-12.png)

  **回归法**可以发现两个相关变量之间的变化模式，通过使数据适合一个函数来平滑数据，即利用拟合函数对数据进行平滑。最常用的回归方法包括线性回归、非线性回归等。图11-13中给出了一个利用线性回归方法来平滑噪声数据的例子。线性回归方法可以找出适合两个变量的“最佳”直线，使一个变量能够预测另一个。而从图11-13中可以看到，大部分数据点是分布在直线y=x+1附近的，而有两个点的偏离距离较大，这两个点就是需要平滑掉的噪声数据。

  ![图11-13 回归方法平滑噪声数据](图11-13.png)

  ### 11.4 数据集成

  数据集成就是将多个数据源中的数据结合起来存放在一个一致的数据存储中。在数据集成的过程中，通常需要考虑多信息源的匹配、数据冗余、数据值冲突等问题(图11-14)。

  ![图11-14 数据集成](图11-14.png)

  #### 11.4.1 多信息源的匹配

  在将不同源头的数据集成到一起的过程中，需要完成各信息源的匹配，即从多信息源中识别现实世界的实体，并进行匹配。这是一个非常复杂的问题，比如，如何确定一个数据库中的id和另一个数据库中的customer_id所指的实体是否同一个实体呢?有的时候需要借助元数据(即数据的数据)的帮助，从而避免在数据集成中发生错误。

  让我们考察图11-15和图11-16中的数据，若用户希望发现客户背景和客户购买类型、购买力的关系，针对数据挖掘的需要，数据预处理时需要将两张表集成为一个数据挖掘源。

  可以看到，在图11-15有一个数据项“id”，而图11-16中有一个数据项名为“customer_id”。如果注意观察该属性的说明，可以看到，这两项数据都是指示客户标志，因而可能是属于同一个属性。但两者的数据类型却不同，“id”属于short int型，而“customer_id”则属于int型。在对这两个数据源进行集成时，需要采用可靠的手段来确定id和customer_id是否是同一个属性。当然，可以借助元数据的帮助。同时，必须把两者的数据统一为相同的类型。

  | 属性名称 | Id        | Gender  | Birth    | Type     | income     |
  | -------- | --------- | ------- | -------- | -------- | ---------- |
  | 数据类型 | Short int | Boolean | Date     | Boolean  | Short int  |
  | 说明     | 客户标志  | 性别    | 出生日期 | 是否会员 | 月收入(元) |

  图11-15 客户基本情况

  | Customer_id | Time     | Goods    | Price    | Count     | Total price |
  | ----------- | -------- | -------- | -------- | --------- | ----------- |
  | Int         | Date     | String   | Real     | Short int | real        |
  | 客户标志    | 交易日期 | 商品名称 | 商品价格 | 商品数量  | 总价格      |

  图11-16 客户交易数据

  #### 11.4.2 冗余数据的处理

  冗余数据是指重复存在的数据。数据冗余的存在使挖掘程序需要对相同的信息进行重复处理，从而增加了数据挖掘的复杂性，导致了挖掘效率的降低。

  主要的数据冗余问题包括：

  1. **属性冗余**：一个属性可能由一个或多个其他属性导出。
  2. **属性或维命名的不一致**，导致数据集成中的冗余。

  来看图11-16。在数据表中，total_price这个属性，实际上可以通过price和count两个属性计算得到，这样就产生了属性冗余。

  要发现数据冗余问题，一般可以采用相关性分析方法。常用的相关性分析方法有x²检验法(Chi-square test)。其中x²的计算方式如下：

  \[ x^2 = \sum \frac{(O_i - E_i)^2}{E_i} \]

  x²越大，则表示变量间的相关性越强。

  **例11.3** 数据表如图11-17中所示。运用卡方检验法判断玩不玩象棋与喜不喜欢看科幻电影之间是否存在相关性。

  |                  | 玩象棋 | 不玩象棋 | 总计 |
  | ---------------- | ------ | -------- | ---- |
  | 喜欢看科幻电影   | 100    | 50       | 150  |
  | 不喜欢看科幻电影 | 200    | 150      | 350  |
  | 总计             | 300    | 200      | 500  |

  图11-17 数据表

  **解答**：

  第一步：建立检验假设。

  H0: 玩不玩象棋与喜不喜欢看科幻电影没有关系
  H1: 玩不玩象棋与喜不喜欢看科幻电影有关系
  评判阈值：α=0.05

  第二步：计算期望值(TRC)，计算公式如下：

  \[ T_{RC} = \frac{n_R \cdot n_C}{n} \]

  式中，TRC表示第R行第C列格子的期望值，nR为期望值同行的合计数，nC为期望值同列的合计数，n为总例数。

  根据公式计算得出每行每列的期望值如下：

  第1行1列：\( \frac{450 \times 300}{1500} = 90 \)
  第1行2列：\( \frac{450 \times 1200}{1500} = 360 \)
  第2行1列：\( \frac{1050 \times 300}{1500} = 210 \)
  第2行2列：\( \frac{1050 \times 1200}{1500} = 840 \)

  将期望值写入数据表中，如图11-18所示。

  |                  | 玩象棋 | 不玩象棋 | 总计 |
  | ---------------- | ------ | -------- | ---- |
  | 喜欢看科幻电影   | 90     | 360      | 450  |
  | 不喜欢看科幻电影 | 210    | 840      | 1050 |
  | 总计             | 300    | 1200     | 1500 |

  图11-18 写入期望值

  第三步：计算x²。

  按照式计算得到：

  \[ x^2 = \frac{(100 - 90)^2}{90} + \frac{(50 - 360)^2}{360} + \frac{(200 - 210)^2}{210} + \frac{(150 - 840)^2}{840} \]

  第四步：查x²表求p值，可知p<0.001，而阈值α=0.05，因此拒绝H0，所以可以认为玩象棋与喜欢看科幻电影之间是有关联的，即存在相关性。

  而相关性的存在一般预示着冗余的存在。

  ### 11.5 数据变换

  所谓数据变换，就是通过变换将数据转换成适合进行处理和分析的形式。数据变换可能涉及如下内容：

  1. **平滑**：去除数据中的噪声(运用分箱、聚类、回归等方法)。
  2. **聚集**：对数据进行汇总和聚集，常采用数据立方体结构，如运用abg()、count()、sum()、min()、max()等对数据进行操作。
  3. **数据概化**：使用概念分层，用更高层次的概念来取代低层次“原始”数据。主要原因是在数据处理和分析过程可能不需要那么细化的概念，它们的存在反而会使数据处理和分析过程花费更多时间，增加了复杂度。例如，Street可以概化为较高层的概念，如city或country；Age可以概化为较高层概念。
  4. **规范化**：将数据按比例缩放，使之落入一个小的特定区间。
  5. **属性构造**：由给定的属性、构造添加新的属性，帮助提高数据处理和分析的精度，以及对高维数据结构的理解。比如根据属性height和width可以构造area属性。通过属性构造，可以发现关于数据属性间联系的丢失信息，这对知识的发现是有用的。

  #### 11.5.1 数据规范化

  数据规范化是指将数据按比例进行缩放，使之落入一个小的特定区域，以加快训练速度，消除数值型属性因大小不一而造成数据处理和分析结果的偏差。例如，可以将工资收入属性值映射到[-1.0,1.0]范围内。常用的规范化方法有：

  1. **最小一最大规范化 (Min-max Normalization)**。
  2. **均值规范化 (Z-score Normalization)**。
  3. **小数定标规范化 (Normalization by Decimal Scaling)**。

  ##### 11.5.1.1 最小一最大规范化

  最小一最大规范化一般适用于已知属性的取值范围，要对原始数据进行线性变换，将原取值区间[min,max]映射到[new_min,new_max]上。其计算公式为：

  \[ v' = \frac{v - \text{min}}{\text{max} - \text{min}} \times (\text{new\_max} - \text{new\_min}) + \text{new\_min} \]

  **例11.4** 映射最小和最大值为12000和98000到区间[0.0,1.0]。试问，值73600和100000在新区间的值为多少?

  **解答**

  根据式，有

  对于值73600：

  \[ v' = \frac{73600 - 12000}{98000 - 12000} \times (1.0 - 0.0) + 0.0 = \frac{61600}{86000} \approx 0.7163 \]

  对于值100000：

  由于100000超出了原始数据的最大值98000，所以不能直接应用公式。如果需要处理这种情况，可以设定一个上限值或者进行截断处理。

  ##### 11.5.1.2 零—均值规范化

  这种方法对属性的值基于其平均值和标准差进行规范。当属性的最大和最小值未知，或孤立点左右了最大一最小规范化时，该方法有用。

  计算方式为：

  \[ v' = \frac{v - \mu}{\sigma} \]

  其中，μ是属性的平均值，σ是属性的标准差。

  **例11.5** 假设属性income的均值与标准差分别为54000元和16000元，使用零均值规范化方法将73600元的属性income值映射为多少?

  **解答**

  依据式，有

  \[ v' = \frac{73600 - 54000}{16000} = \frac{19600}{16000} = 1.225 \]

  ##### 11.5.1.3 小数定标规范化

  该方法通过移动属性值小数点的位置进行规范化。小数点的移动位数依赖于属性值的最大绝对值。

  其计算方式为：

  \[ v' = \frac{v}{10^j} \]

  式中，j是满足下式的最小整数：

  \[ \max(|v'|) < 1 \]

  **例11.6** 假定A的值由-986到917，确定小数定标规范化的系数j的大小。

  **解答**

  由于A的最大绝对值为986，因此若需要保证

  \[ \max(|v'|) < 1 \]

  则j的取值为3。因此，A被规范到[-0.986, 0.917]的范围之内。

  ##### 11.5.1.4 数据规范化的注意事项

  数据规范化对原来的数据改变很多，尤其是零一均值规范化和小数定标规范化。一定注意，要保留规范化参数，以便将来的数据可以用一致的方式规范化。

  ### 11.6 数据归约

  之所以要进行数据归约，是因为被分析的对象数据集往往非常大，分析与挖掘会特别耗时甚至不能进行。而通过数据归约处理，可以减少对象数据集的大小。数据归约技术能够从原有的庞大数据集中获得一个精简的数据集合，并使这一精简的数据集保持原有数据集的完整性，以提高数据挖掘的效率。

  因此，对于数据归约技术有如下要求：

  1. 所得归约数据集要小。
  2. 归约后的数据集仍接近于保持原数据的完整性。
  3. 在归约数据集上所得分析结果应与原始数据集相同或基本相同。
  4. 归约处理时间少于挖掘所节约的时间。

  数据归约的策略一般有以下几种：

  1. **数据立方体聚集**：结果数据量小，不丢失分析任务所需信息。
  2. **维归约**：检测并删除不相关、弱相关或冗余的属性。
  3. **样本归约**：从数据集中选出一个有代表性的样本的子集。子集大小的确定要考虑计算成本、存储要求、估计量的精度以及其他一些与算法和数据特性有关的因素。
  4. **特征值归约**：即特征值离散化技术，它将连续型特征的值离散化，使之成为少量的区间，每个区间映射到一个离散符号。这种技术的好处在于简化数据描述，并易于理解数据和最终的挖掘结果。

  #### 11.6.1 数据立方体聚集

  所谓数据立方体，就是一类多维矩阵，让用户从多个角度探索和分析数据集，通常是一次同时考虑三个因素(维度)。

  当试图从一堆数据中提取信息时，需要工具来帮助找到那些有关联的和重要的信息，以及探讨不同的情景。一份报告，无论是印在纸上的还是出现在屏幕上，都是数据的二维表示，是行和列构成的表格。只需要考虑两个因素，但在真实世界中需要更强的工具。

  数据立方体是二维表格的多维扩展，如同几何学中立方体是正方形的三维扩展一样。“立方体”这个词让人们想起三维的物体，也可以把三维的数据立方体看作一组类似的互相叠加起来的二维表格。

  但是数据立方体不局限于三个维度。大多数在线分析处理(OLAP)系统能用很多个维度构建数据立方体，例如，微软的SQL Server 2000 Analysis Services工具允许维度数高达64个(虽然在空间或几何范畴想象更高维度的实体还是个问题)。

  在实际中，常常用很多个维度来构建数据立方体，但人们倾向于一次只看三个维度。数据立方体之所以有价值，是因为人们能在一个或多个维度上给立方体做索引。图11-19为数据立方体的示例。

  在数据立方体中，存放着多维聚集信息。每个单元存放一个聚集值，对应于多维空间的一个数据点。每个属性可能存在概念分层，允许在多个抽象层进行数据分析。最底层的数据立方体称为基本方体，最高层抽象的为顶点方体，不同层创建的数据立方体称为方体。每个数据立方体可以看作是方体的格。

  ![图11-19 数据立方体的示例](图11-19.png)

  通过对数据立方体的聚集操作，可以实现数据的归约。在具体操作时有多种方式，比如，既可以针对数据立方体中的最低级别进行聚集，也可以针对数据立方体中的多个级别进行聚集，从而进一步缩小处理数据的尺寸。在具体操作时，应该引用适当的级别，便于问题的解决。

  图11-20中左半部分所示数据是某商场2000—2002年每季度的销售数据。在对其进行数据分析时，可以对数据立方体进行聚集，使结果数据汇总每年的总销售额，而不是每季度的总销售额。如图11-20中右半部分所示，从图中可以看出，聚集后数据量明显减少，但没有丢失分析任务所需要的信息。当感兴趣的是季度销售的总和或者年销售时，由于有了聚集值，可以直接得到查询的结果。

  ![图11-20 数据立方体的聚集](图11-20.png)

  #### 11.6.2 维归约

  人们收集到的原始数据所包含的属性往往很多，但是大部分与所需要开展的挖掘任务无关。例如为了对观看广告后购买新款CD的顾客进行分类，收集了大量数据，所要开展的分析与年龄和顾客个人喜好有关，但通常与顾客电话号码属性无关(图11-21)。

  冗余属性的存在会增加要处理的数据量，减慢挖掘进程。维归约，就是指通过删除不相关的属性来减少数据挖掘要处理的数据量的过程。例如，挖掘学生选课与所取得的成绩的关系时，学生的电话号码可能与挖掘任务无关，可以去掉。

  维归约一般可以采用属性子集选择和主成分分析法来实现。

  ![图11-21 维归约](图11-21.png)

  ##### 11.6.2.1 属性子集选择

  所谓属性子集选择，是指在初始的N个属性中选择出一个有m(m<N)个属性的子集，这m个属性可以如原来的N个属性一样用来正确区分数据集中的每个数据对象。这里有几个关键点要注意：

  1. 新选择出的属性数要少于原始属性数。
  2. 新选择出的属性可以和原来的N个属性一样描述数据集。
  3. 新选择出的属性可以和原来的N个属性一样区分数据集中的数据对象。

  属性选择的基本步骤如下：

  1. 子集产生。
  2. 子集评估。
  3. 根据预先确定的停止准则终止过程。
  4. 结果有效性验证。

  首先来看子集产生过程。子集产生过程是一个搜索过程，它产生用于评估的属性子集。对于含有N个属性的属性集合，它的子集共有2^N个，如何从这2^N个子集中选择一个合适的子集?

  一般采用启发式方法来实现子集选择，常用的方法有：

  1. **逐步向前选择(Step-wise Forward Selection)**：由空属性集合开始，选择原属性集中最好的属性，并将它添加到该集合中，如此迭代循环。这种方法精确性更高，但计算更多。
  2. **逐步向后删除(Step-wise Backward Elimination)**：从整个属性集开始，删掉其中最坏的属性，如此迭代循环。
  3. **向前选择与向后删除的结合**。
  4. **决策树归纳(Decision-tree Induction)**：构造一个类似于流程图的结构，每个内部节点(非树叶)表示一个属性上的测试，每个分枝对应于测试的一个输出；每个外部节点(树叶)表示一个判定类。在每个节点，选择最好的属性，将数据划分成类。当决策树归纳用于属性子集选择时，由给定的数据构造决策树。不出现在树中的所有属性假定是不相关的，那么出现在树中的点会属性形成归约后的属性子集。

  子集产生过程所生成的每个子集都需要用事先确定的评估准则进行评估，并且与先前符合准则最好的子集进行比较，如果它更好一些，那么就用它替换前一个最优的子集。如果没有一个合适的停止规则，在属性选择进程停止前，它可能无穷无尽地运行下去。

  属性选择过程可以在满足以下条件之一时停止：

  1. 达到一个预先定义所要选择的属性数。
  2. 达到预先定义的迭代次数。
  3. 增加(或删除)任何属性都不产生更好的子集。

  最后，选择的最优子集需要通过在所选子集和原属性集进行不同的测试和比较，使用人工和现实世界的数据集对产生的结果进行有效性验证。

  ##### 11.6.2.2 主成分分析

  主成分分析(Principal Components Analysis, PCA)，是目前最流行的大型数据集归约的统计学方法。主成分分析，通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫作主成分。

  在用统计分析方法研究多变量的课题时，变量个数太多就会增加课题的复杂性。人们自然希望变量个数较少而得到的信息较多。很多情形之下，变量之间是有一定的相关关系的，当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。主成分分析是在原先提出的所有变量中将重复的变量(关系紧密的变量)删去，建立尽可能少的新变量，使这些新变量两两不相关，而且在反映课题的信息方面尽可能保持原有的信息。

  可以这样去描述主成分分析方法：一个m维向量样本集X={x₁, x₂, x₃, …, xₘ}通过主成分分析法会被转换成另外一个相同维度的集Y={y₁, y₂, y₃, …, yₘ}，其中，Y的前几维中包含了大部分的信息内容——这样可以以低信息损失将数据集减小到较小的维度。

  主成分分析所得到的主成分与原始变量之间存在如下关系：

  1. 主成分保留了原始变量绝大多数信息。
  2. 主成分的个数大大少于原始变量的数目。
  3. 各个主成分之间互不相关。
  4. 每个主成分都是原始变量的线性组合。

  属性子集选择是通过保留原属性集的一个子集来减小属性集，而主成分分析则是通过创建一个替换的、更小的变量集来组合属性的基本要素，从而使原数据可以投影到该较小的集合中。主成分分析常常能够揭示先前未曾察觉的联系，并因此允许解释不寻常的结果。

  #### 11.6.3 特征值归约

  特征值归约又称特征值离散化技术，它将连续型特征的值离散化，使之成为少量的区间，每个区间映射到一个离散符号。这种技术的好处在于简化了数据描述，易于理解数据和最终的挖掘结果。

  特征值归约可以是有参数的，也可以是无参数的。有参数方法是指，使用一个模型来评估数据，只需存放参数，而不需要存放实际数据。

  有参数的特征值归约方法有以下两种：

  1. **回归**：包括线性回归和多元回归。
  2. **对数线性模型**：近似离散多维概率分布。

  无参数的特征值归约方法有三种：

  1. **直方图**：采用分箱近似数据分布，其中V-最优和MaxDiff直方图是最精确和最实用的。
  2. **聚类**：将数据元组视为对象，将对象划分为群或聚类，使在一个聚类中的对象“类似”，而与其他聚类中的对象“不类似”，在数据归约时用数据的聚类代替实际数据。
  3. **抽样**：用数据的较小随机样本表示大的数据集，如简单抽样N个样本(类似样本归约)、聚类抽样和分层抽样等。
