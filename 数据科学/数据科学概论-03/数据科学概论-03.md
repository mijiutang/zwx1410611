 # 第 3 章  OLAP 与结构化数据分析

## 3.1  联机分析处理(OLAP)与结构化数据分析  

联机分析处理 (Online Analytic Processing, OLAP, 也称为在线分析处理) 是在以星型模型(或者雪花模型)建模的数据仓库上进行多维分析。多维分析指的是从各个角度对我们感兴趣的一些数量进行汇总分析，比如我们从地区、客户、时间等维度，对销售明细数据进行汇总分析。

结构化数据分析是一个(包含联机分析处理)更为宽泛的概念。它运行在结构化数据上(一般以关系数据库表进行建模)，分析可以表达成一个SQL聚集查询。数据库的模式没有必要按照星型模型进行严格建模，查询可以是一个简单的汇总查询，没有必要从多个角度进行复杂汇总。比如，我们需要从销售明细表上分析出最近一段时间内销售量最大的前5种产品。我们直接按照产品ID进行销售明细数据分组，汇总销售额，然后按照汇总的销售额进行排序，提取前5种产品即可。

本章介绍传统的联机分析处理技术，以及近年来兴起的SQL on Hadoop技术。

### 3.1.1 从操作型的业务数据库向数据仓库抽取、转换和装载数据

大量的业务系统采用关系数据库来进行数据管理，随着业务的不断发展，各个企事业单位和政府部门积攒了大量的业务数据。通过分析这些数据，可以展示数据中隐藏的规律，指导业务决策。

业务系统的后台数据库需要支持日常业务的持续运行，数据处理任务的响应时间要求一般比较短，这类数据处理任务称为联机事务处理 (OLTP) 。比如在银行里，存款、取款、查账、转账的业务，要求的响应时间一般是几秒钟。

数据分析任务一般在大量数据上(甚至在整个数据库的数据上)运行，以便获得宏观的汇总信息和分析结果。这类任务消耗大量资源，人们对其响应时间要求没有那么紧迫，一般以分钟或小时计。为了避免这类数据处理任务占用过多资源，对日常业务的顺利运行产生干扰，一般在业务数据库之外建立数据仓库系统。它从业务数据库抽取、转换和装载 (Extract Transform and Load, ETL) 数据，支持人们在上面执行各种分析任务。

### 3.1.2 数据仓库与星型模型

Bill Inmon认为，数据仓库是面向主题的 (Subject Oriented)、集成的 (Integrated)、非易失的 (Non Volatile) 和时变的 (Time Variant) 数据集合，用以支持管理决策。面向主题的是指每个数据仓库对应于企事业单位决策所包含的所有分析对象(数据)。集成的是指数据仓库按照决策主题选择数据，把分布在各个部门中的多个异构数据源的数据集成起来，并且以新的数据模型来存储。非易失的是指数据仓库的数据装载以后，一般不会删除。时变的是指随着业务的发展，新的业务数据不断地被抽取和装载到数据仓库中，以便进行分析。数据仓库一般不对应到某个厂商的具体产品，而是指一种面向分析的数据存储方案。

业务数据库上主要执行操作型处理任务，数据仓库上主要执行分析型处理任务。业务数据库的新数据不断通过抽取、转换、装载过程，装载到数据仓库中，这就是企事业单位采用的数据处理基础设施架构。虽然某些数据库系统，比如 SAP HANA内存数据库等，宣称支持OLTP和OLAP混合负载，但是它主要应用于分析型应用场合，并支持数据的快速更新，一般不会把它当作运行操作型业务的基础数据库平台，同时在上面运行大量的分析负载。表3-1展示了操作型数据处理和分析型数据处理的主要区别。

**表3-1 操作型数据处理和分析型数据处理的差别**

| 比较项目         | 操作型数据处理        | 分析型数据处理                         |
| ---------------- | --------------------- | -------------------------------------- |
| 数据模型         | 实体-关系模型(ER模型) | 星型模型和雪花模型                     |
| 操作的记录数量   | 少量记录              | 大量记录                               |
| 数据是否可以更新 | 数据可以更新、删除    | 一般只对数据进行追加、不删除、极少更新 |
| 响应时间要求     | 秒级                  | 分钟级、小时级                         |
| 目的             | 支持业务运行          | 支持决策需求                           |

数据仓库主要采用星型模型进行数据建模。在这个模型中，包含事实表和维表。事实表记录具体的业务交易，比如记录客户的购物信息。维表记录分类信息，比如时间信息、地理区域信息、产品分类信息等。

每个维表代表人们观察数据的一个角度。一般维表具有层次结构 (Hierarchy)，即人们观察事物的不同细节，比如时间维度包括年、季度、月份、日期等不同细节的层次 (Level)。维表的具体的一个取值称为维的成员 (Member)。比如，某年某月某日是时间维(时间维的最低层次是日期)的一个成员。事实表通过外键 (Foreign Key) 和维表关联起来，它除了和维表建立联系之外，更重要的是它记录了业务的一些度量 (Measure) 信息。比如，购物信息里的商品单价、商品数量、价格小计等，这些数值型属性称为度量值。下面我们介绍SSB测试基准的数据模型，该模型为一个典型的星型模型。

SSB(Star Schema Benchmark) 是麻省州立大学波士顿校区的研究人员研发的基于现实商业应用的数据仓库测试基准。该测试基准被学术界和工业界广泛接受，用来测试决策支持类应用中的数据库系统性能。

这个测试基准包含数据模型、工作负载以及性能指标三个方面。图3-1显示了该测试基准的数据模型，包含一个事实表和四个维表。事实表LineOrder记录订单信息，维表Customer记录客户信息，维表Part记录配件信息，维表Date记录时间信息(具有年/月/日概念层次关系)，维表Supplier记录供应商信息。图3-1中，每个数据库表的名称后面的括号里是字段名的前缀，比如LineOrder表的字段名的前缀为LO_，那么OrderDate字段的全名应该为LO_OrderDate。

事实表通过外键和维表建立主外键关系，包括通过外键lo_custkey和Customer的主键c_custkey建立主外键关系，通过外键lo_partkey和Part表的主键p_partkey建立主外键关系，通过外键lo_suppkey和Supplier表的主键s_suppkey建立主外键关系，以及通过外键lo_orderdate和时间维表DwDate的主键d_datekey建立主外键关系等。

```
图 3 - 1 Star Schema Benchmark(SSB) 数据模型
```

### 3.1.3 联机分析处理 (OLAP)

数据仓库模型建好并且装载数据以后，我们就可以对数据进行分析。数据仓库上的分析任务包括简单分析和复杂分析。简单分析指的是利用数据生成报表以及进行多维分析。复杂分析指的是在数据上运行复杂的统计方法、机器学习和数据挖掘算法，从而发现不是那么显然的规律，发现新的知识。本章主要讲述业务数据上的简单分析。

在这里，简单分析主要是指联机分析处理，包括固定报表和多维分析，其表现形式是在数据仓库数据上执行查询获得汇总信息。由于这些分析无须在数据上进行多次扫描和执行复杂的处理逻辑，所以其响应时间得到很好的控制，在高性能的数据仓库系统上，比如SAP HANA以及MonetDB，甚至可以获得秒级的响应时间。图3-2展示了操作型数据库、数据仓库及其上面的工作负载。

```
操作型数据处理	分析型数据处理	
数据服务与联机事务处理	简单分析
(联机分析处理)	复杂分析(统计分析、 数据挖掘、机器学习)

抽取、转换、装载 业务数据库
图3-2 操作型数据库与数据仓库
```

联机分析处理((LAP)是数据仓库的主要负载和应用，通过分析操作，为高层管理人员提供决策支持。OLAP系统根据分析人员的要求，快速灵活地进行大量数据的复杂查询处理(虽然是复杂查询处理，但是相对于数据挖掘来讲，其处理逻辑仍然是简单的)。OLAP系统的前端软件把查询结果以直观的图表方式提供给决策人员，方便他们掌握业务状况，做出正确的决策。

联机分析处理的主要操作包括下钻 (Drill Down)、上卷 (Drill Up)、切片 (Slice)、切块 (Dice)、旋转 (Pivot)等。下钻和上卷是改变维的层次，变换分析的粒度。上卷是在某个分析维度上，将低层次的细节数据概括到高层次的汇总数据。下钻是在某个分析维度上，从高层次的汇总数据深入到细节数据进行观察。比如当我们把商品销售情况表 (SSB数据模型中的Line(rder表)的一部分记录在时间维度上按照月度进行分组汇总(比如销售额)以后，上卷操作则把分析的粒度变成年度，即按照年度进行分组汇总，下钻操作则把分析的粒度变成日期(即以天作为汇总的粒度)。切片和切块是选定一部分维度值，然后查看度量数据在剩余维度上的分布情况。如果剩余的维度只有两个则称为切片，如果剩余维度有三个或者以上称为切块。比如在SSB模型中，我们选定某个(些)顾客和供应商后，对销售数据在剩余的两个维度即配件维度、时间维度上进行汇总，就是切片操作。如果我们只选定某个(些)顾客，对销售数据在剩余的三个维度，即供应商维度、配件维度、时间维度上进行汇总，就是切块操作。

旋转操作是变换维的方向，也就是在汇总表格中(在Excel中称为数据透视表)，重新安排维的位置，即对行/列进行互换。比如在表3-2中，我们对销售数据进行了时间、供应商、配件三个维度的汇总。我们选择配件维度作为行方向的维度，时间维度和供应商维度作为列方向的维度。假设我们仅2010年的4个季度进行销售额汇总，选择了两个供应商和三个配件。为了简化表格设计，我们把合计数和行方向、列方向的维度的成员放在一起。通过这张表(见表3-2)，我们方便地观察到2010年不同季度、不同供应商、不同配件的销售额，而且得到各个配件的合计数、各个季度的合计数，以及各个季度之下的各个供应商的合计数。

**表3-2 商品销售额汇总表a**

|                 | 2010年第1季度 合计300 | 2010年第2季度 合计333 | 2010年第3季度 合计306 | 2010年第4季度 合计224 |
| --------------- | --------------------- | --------------------- | --------------------- | --------------------- |
| 供应商1 合计105 | 供应商2 合计195       | 供应商1 合计108       | 供应商2 合计225       | 供应商1合计111        |
| 配件1 合计42    | 35                    | 65                    | 36                    | 75                    |

续前表

|                 | 2010年第1季度 合计300 | 2010年第2季度 合计333 | 2010年第3季度 合计306 | 2010年第4季度 合计224 |
| --------------- | --------------------- | --------------------- | --------------------- | --------------------- |
| 供应商1 合计105 | 供应商2 合计195       | 供应商1 合计108       | 供应商2 合计225       | 供应商1合计111        |
| 配件2 合计421   | 35                    | 65                    | 36                    | 75                    |
| 配件3 合计421   | 35                    | 65                    | 36                    | 75                    |

注：表中的销售额数据仅供参考，非实际数据。

接着，我们把时间维度做行/列互换，把它作为行方向的维度，重新观察数据，新的汇总表如表3-3所示。通过行/列互换以后，我们仍然观察到行列互换之前的各个Cell的合计数，还得到两个供应商的合计数、各个季度的合计数，以及各个季度下各个配件的合计数等。

**表3-3 商品销售额汇总表 b**

|                       | 供应商1 合计423 | 供应商2 合计840 |
| --------------------- | --------------- | --------------- |
| 2010年第1季度 合计300 | 配件1,合计100   | 35              |
|                       | 配件2,合计100   | 35              |
|                       | 配件3,合计100   | 35              |
| 2010年第2季度 合计333 | 配件1,合计111   | 36              |
|                       | 配件2,合计111   | 36              |
|                       | 配件3,合计111   | 36              |
| 2010年第3季度 合计306 | 配件1,合计102   | 37              |
|                       | 配件2,合计102   | 37              |
|                       | 配件3,合计102   | 37              |
| 2010年第4季度 合计324 | 配件1,合计108   | 33              |
|                       | 配件2,合计108   | 33              |

| | 配件3.合计108 | 33 | 75 |

需要指出的是，我们不仅能够对汇总表进行行列互换，而且可以对行方向的维度进行顺序调整，比如对表3-3的时间和配件的顺序进行调整，我们将得到各个配件的销售额的合计数，各个配件在各个季度的销售额的合计数等。

### 3.1.4 三种类型的OLAP 系统

OLAP系统以数据仓库作为基础，从数据仓库中提取详细数据的一个子集，经过聚集操作，生成汇总结果(也可以事先计算汇总结果)，供前端分析工具读取和展现。实现该功能的软件称为OLAP服务器。

按照数据存储格式分类，OLAP系统可以分为多维OLAP(Multi Dimensional OLAP, MOLAP)、关系OLAP(Relational OLAP, ROLAP)以及混合OLAP(Hybrid OLAP, HOLAP)三类。

- **MOLAP**：将OLAP分析所用到的多维数据，在物理上存储为多维数组的形式，形成“立方体”(Cube)的结构。星型模型的各个维的属性值，被映射成多维数组的下标值或下标的范围，聚集数据(即汇总数据)作为多维数组的值存储在数组的单元中。由于MOLAP采用了新的存储结构，从物理层实现OLAP，因此又称为物理OLAP(Physical OLAP)。MOLAP对数据进行预先汇总，由于实际应用中的星型模型存在若干维度，比如销售数据仓库的星型模型包含地区、时间、产品等维度，此外，有些维度还存在层次结构，比如时间维度存在“年-季度-月-日”的层次结构，在哪些维度、在什么维度层次上进行数据的预先汇总，这样的组合是非常多的，每种组合生成的汇总数据，称为一个Cuboid。如果我们在各种组合上都事先计算汇总数据，其好处是进行多维分析时分析速度快，但消耗的存储空间非常大。
- **ROLAP**：将多维数据存储在关系数据库中，通过把OLAP操作表达成SQL查询的形式，在关系数据库上执行，获得汇总结果。对于一些使用频率比较高、计算工作量比较大的查询，可以把它定义为物化视图，以提高查询的性能。物化视图(Materialized View)是把查询的结果存储到关系数据库表中，以备用户再次发起该查询时，无须从明细数据进行扫描和汇总，直接从物化视图进行查询即可。ROLAP通过一些软件工具或中间软件实现，物理层仍采用关系数据库的存储结构，因此称为虚拟OLAP(Virtual OLAP)。
- **HOLAP**：MOLAP和ROLAP有着各自的优点和缺点，结构迥异，给开发人员设计OLAP系统造成了困难。为此，人们提出一个新的OLAP结构——混合OLAP(HOLAP)，它把MOLAP和ROLAP两种技术结合起来，是这两种技术优点的有机结合，能满足用户各种复杂的OLAP分析请求。HOLAP基于混合数据组织实现OLAP，比如低层数据是关系型的，高层数据是多维数组型的，也就是将细节数据保留在关系型数据库的事实表中，但是聚集后的数据保存在“立方体”(Cube)中，这种方式具有更大的灵活性。HOLAP的查询效率比ROLAP高，但低于MOLAP。

## 3.2 高性能OLAP系统的关键技术  

### 3.2.1 列存储技术

在关系数据库的表格里，每个记录包含若干属性列的值。如果数据库表按照行存储格式保存(Row-wise Storage)，那么在每个数据块(或者页面)里面，每行记录的各个属性列的值依次存放。如果数据块的大小，小于一个记录的大小，那么存储一个记录可能需要超过一个数据块的空间。如果一个数据块的大小，大于一个记录的大小，那么一个数据块就可能存放多个记录。

在联机事务处理(OLTP)类应用中，大部分的事务存取少量的记录，但是需要读取和写入这些记录的所有(或者大部分)属性列。行存储对于OLTP类应用是一种优化的存储结构。

需要指出的是，在面向OLTP应用的数据库系统中，数据以页面为单位存储在磁盘里面，页面的大小一般是8KB, 16KB等。在面向数据分析的大数据管理系统中，比如Hadoop，数据以数据块为基本单位保存在磁盘中，数据块的大小一般为64MB, 128MB, 256MB等。

图3-3展示了数据库表中的记录是怎么按照行存储格式进行存储的。这个数据库表记录了人们的社会安全号、姓名、年龄、所在的州、城市和具体的街道地址等信息，我们假设一个数据块正好存放两行记录。在实际应用中，一般一个数据块(或者页面)可以存放若干记录。

```
SSN	Name	Age	Address	City	State
1012599898	John	44	798 Main ST	Binghamton	NY
892375863	Smith	37	1636 Riverside ST	Syracuse	NY
318370709	Wang	23	455 June ST	Chicago	IL
101259875	Sun	25	385 Clinton ST	Los Angles	CA
……

1012599898John|44 798 Main ST|BinghamtonNY|  892375863|Smith|3711636 Riverside STISyracuse|NYI	3183707091Wang|23 455 June STIChicago IL
101259875|Sun|25385 Clinton STILos Angles|CA
Block 1                                                                 Block 2
图3-3 行存储示意图
```

在面向分析型应用的数据库系统中，一般采用列存储的方式保存数据。我们先了解列存储的基本原理，然后分析它带来的好处。采用列存储，若干记录的某个属性列的值连续存储在一系列数据块中，由此完成某个属性列的存储，对其他属性列，依此办法处理。

对于上述数据库表，因为它包含六列，所以需要六个数据块保存这些属性列，如果一个数据块不能容纳某个属性列的所有值，则需要更多的数据块来保存，如图3-4所示。

```
1012599898 892375863 318370709 101259875…
John|Smith|Wang|Sun…
44|37|23|25|…
798 Main ST|1636 Riverside ST|455 June ST|385 Clinton ST…
Binghamton|Syracuse|Chicago|Los Anglesi…
NYINYIILICA|…
图3-4 列存储示意图
```

使用列存储，带来的一个好处是，由于数据块包含相同数据类型(DataType)的值，我们可以使用数据压缩技术，减少磁盘空间占用和存取这些属性列数据的I/O操作开销，从而加快数据处理过程。

其中最重要的一种压缩技术称为字典编码(Dictionary Based Compression)。一般来讲，在一列数据中总是有些重复值，比如图3-3中用户所属的州，其取值为美国的50个州之一，大量的用户在这个属性的取值会出现很多重复。除此之外，产品名称、产品编码、城市名称等属性列，也有很多的重复值。我们可以建立一个字典，对这些取值分配一个编码，建立编码到取值的对应关系。一般来讲，编码长度(即它占用几个字节)一般比这些属性原来的属性值小得多，于是我们达到了数据压缩的效果。比如有一个属性列，不同取值有上百万个，每个取值的大小从64字节到128字节不等，数据库表总共有500万行，没有压缩之前，其占用的空间为5M×128=640MB。当我们采用字典编码方法，100万个不同值(Distinct Value)，需要四个字节作为编码的长度对其进行唯一编码即可，数据被压缩到5M×4=20MB。选择操作以及连接操作可以通过查询改写，直接在编码以后的字段值上进行操作，无须把编码恢复回原来的数据值。比如“CA”州名，经过编码以后，获得0001的编号，那么“state='CA'”查询条件，就可以改写成“state=0001”。

分析型应用对数据的存取表现出这样的特点，它一般涉及数据库表大量的记录，但是不会对这些记录的所有属性列进行操作，只需存取少部分数据库表的属性列，对它们进行分组和汇总。比如一个数据库表有100个属性列，某个查询只需读取5个属性列，那么它只需存取5%的数据量(相对于全表扫描)就可以完成查询。采用行存储，则95个不需要的属性列也被读进来，查询处理做了很多无用功。采用列存储，相对于行存储，在处理上述查询时只需提取必要的5个属性列数据就可以，需要付出的开销要少得多。

如下简单的聚集查询，目的是把2011年2月2日的所有销售明细提取出来，把每个明细记录的单价和数量相乘，然后累加，获得该日的销售额。

```sql
Select Sum(price*quantity)
From sales
Where sales.date='2011-02-02'
```

当采用行存储，查询的处理模式是，如果数据库表有索引，而且查询选择率(Selectivity)比较低，那么首先扫描索引，取得所有符合条件的记录的ID，然后提取相应的记录，在记录上进行投影，即选取单价(price)以及数量(quantity)列，然后进行汇总，如图3-5所示。由于无关的数据列也被提取出来，当无关数据列的数量还比较多时，查询的效率是比较低的。如果没有合适的索引可以使用，那么需要进行全表扫描，代价就更大了，查询的效率就更低了。

```
图3-5 行存储与查询处理(全表扫描)
```

当采用列存储，查询处理器只需提取日期(date)、单价(price)、数量(quantity)等数据列，而且在提取日期列时，可以应用选择谓词，即使用查询条件“sales.date=2011-02-02”对数据进行过滤，只提取该数据列的一部分，由此获得符合条件的记录(所在的行)的偏移量。根据这些偏移量，继续提取相对应的单价(price)和数量(quantity)两个数据列，进行乘法计算，并进行汇总即可，如图3-6所示。

采用列存储，我们不仅可以节省I/O开销，而且可以节约内存空间，因为与查询无关的属性列都没有被装载到内存中。虽然列存储具有便于压缩、加快查询等优势，但是它也带来一些成本。当我们需要重新把一行记录组装出来时，代价是比较大的。由于隶属于同一行的各个属性列的值保存在不同的数据块中，在查询涉及的数据列比较多的情况下，每个属性列都需要单独的磁盘I/O去提取数据。列存储一般适用于读取少量属性列进行分析的应用场合，而不是经常对某些记录进行整体操作的场合。

```
Scan column date

Column date

Scan Column price

Column price    Column quantity
图3-6 列存储与查询处理
```

在某些应用场合，完全把各个属性列切割开单独存放不一定是最好的方案。对于一些经常一起访问的属性列，比如上述查询中的单价(price)列和数量(quantity)列，把它们组合构成列分组(Column Family)一起存放，可以加快查询处理。

### 3.2.2 位图索引技术

位图(Bitmap)索引是针对数据仓库的索引形式。对于低基数(Cardinality，即不同值的数量)的字段可以建立Bitmap索引。下面以一个用户表为例，展示如何建立性别字段上的Bitmap索引。性别字段只有“男”/“女”两个不同取值，因此它是一个低基数的字段。假设用户表的实际数据如表3-4所示(某些无关字段值未显示)。

**表3-4 Bitmap索引实例**

| 序号 | 用户ID | 用户名 | 性别 | 出生年月 | 其他字段 | “男”Bitmap序列 | “女”Bitmap序列 |
| ---- | ------ | ------ | ---- | -------- | -------- | -------------- | -------------- |
| 1    | 5      | Tom    | 男   | 1980.01  | …        | 1              | 0              |
| 2    | 6      | Mary   | 女   | 1980.02  | …        | 0              | 1              |
| 3    | 7      | June   | 女   | 1979.12  | …        | 0              | 1              |
| 4    | 14     | Mark   | 男   | 1979.11  | …        | 1              | 0              |
| 5    | 15     | Kate   | 女   | 1985.03  | …        | 0              | 1              |
| 6    | 16     | John   | 男   | 1982.02  | …        | 1              | 0              |
| …    | …      | …      | …    | …        | …        | …              | …              |

表3-4的最后两列，分别是针对性别为“男”和性别为“女”的Bitmap索引的具体取值。我们看到，在针对性别取值为“男”的Bitmap索引中，当该记录的性别字段的取值为“男”时，其取值为1，否则为0，每个记录只需一个比特(Bit)保存索引信息。由于Bitmap索引出现大量的连续的0和1，再经过压缩，其空间占用一般很小。

上述Bitmap索引，支持查询条件为“User.sex='男'”这样的查询。通过扫描针对性别字段取值为“男”的Bitmap索引，可以快速定位符合条件的记录为1号、4号和6号记录。

从这个实例可以看出，当我们为某个字段建立Bitmap索引时，Bitmap序列的个数是字段的不同取值的个数(Number of Distinct Value)，未压缩情况下每个字段值对应的Bit列表的Bit数量为记录的个数。假设记录数为n，字段A的基数为C，为字段A建立的Bitmap索引的大小为n×C/8字节。当C很大，即字段的基数较大时，Bitmap索引的大小急剧膨胀，建立Bitmap索引的代价相对于其所获得的收益变得不合适了。此外，标准的Bitmap索引并不支持范围查询。

对于某些范围查询字段，其基数(即不同的取值)是巨大的，直接使用上述Bitmap索引方法，不仅空间开销大，而且并未支持范围查询，于是人们提出了分容器的Bitmap索引的思想。其实现机制是：把字段的值域[Vlow, Vhigh]划分成一个个的片段，每个片段内包含一定的记录数量，称为容器(Bin)，各个容器形成首尾相连的关系。

建立Bitmap索引时，不是为字段的每个不同值建立索引，而是针对每个容器建立索引，也就是对整个值域的一个子集范围进行索引。当执行范围查询“V₁<=A;<=V₂”时，对于完全处在[V₁, V₂]之间的片段，直接提取其Bitmap索引，即可获得对应的记录。对于与[V₁, V₂]部分重叠的片段，根据其Bitmap索引提取记录后，还需具体判断字段值是否真正落在[V₁, V₂]范围内。

比如，对于资产总额字段，我们建立了(0,10),(10,20), …,(90,100)等片段(假设最大资产数量是100万美元)。当查询“25<=总资产<=56”的用户时，[30,40),(40,50)两个片段对应的记录肯定符合条件，把两者的Bitmap索引进行或(Bit-wise OR)操作，就可以寻访到符合条件的记录。(20,30),(50,60)两个片段包含的记录则有可能符合条件，也可能不符合条件，把两者的Bitmap索引进行或操作，寻访到相应记录后，还需确认该记录是否符合“25<=总资产<=56”的条件，如图3-7所示。

```
表格数据

用户ID 姓名 性别 出生年月  总资产 其他字段 总资产区间
5   Tom 男   1980.01 22万美元         (20,30)
6   Mary 女   1980.02 57万美元 …      (50,60)
7  June   女   1979.12 33万美元  …      [30,40]
14  Mark  男   1979.11 25万美元  …      [20,30]
15  Kate  女   1985.03 42万美元  …     (40,50)
16  John  男   1982.02 53万美元  …      (50,60)

总资产字段的Bitmap索引

查询条件“25<=总资产<=56”对应的范围
图3-7  分容器 Bitmap 索引实例
```

如果对字段的值域分割的粒度过大，那么在查询条件和片段部分覆盖情况下，需要提取和判断的记录数量就越多。如果值域分割的粒度过细，那么需要为更多的片段建立Bitmap索引，造成更大的索引空间开销，需要折中考虑。

### 3.2.3 内存数据库技术

随着内存价格的下降，我们可以为服务器安装更大的内存。对于中小规模的数据库来讲，我们可以把数据库的所有数据都装载到内存。内存数据库(Main Memory Database, MMDB)和磁盘数据库(Disk Resident Database, DRDB)的主要区别在于其数据的主副本在内存中，磁盘等外存仅仅作为数据备份的设备。

内存数据库分为面向OLTP应用的内存数据库和面向OLAP应用的内存数据库两类。面向OLTP应用的内存数据库，有来自IBM的SolidDB数据库、来自Oracle的Timesten数据库，以及VoltDB等。面向OLTP应用的内存数据库支持高速的事务处理，应用于电信业务、股票期货交易撮合、工业控制等需要实时(Real-Time)响应的应用场合。面向OLAP应用的内存数据库，有SAP HANA数据库、Vectorwise数据库、MemSQL数据库等。面向OLAP应用的内存数据库，支持快速的数据查询和分析。下面我们从存储、索引、查询优化、并发控制、恢复技术等方面，介绍内存数据库的相关技术。

1. **存储技术**

传统数据库系统，围绕磁盘来进行设计。数据的主副本保存在磁盘中，在需要存取时装载到内存缓冲区(Memory Buffer)。在内存数据库中，数据的主副本保存在内存中，暂时不用的数据保存到磁盘，以便为将要用到的数据腾出空间。围绕内存重新对数据库系统进行设计的策略称为Anti-Caching策略。一旦数据完全保存在内存中，如何快速有效地在内存和CPU Cache之间进行数据交换，成为性能优化的关键点。优化的目的就是提高CPU的指令缓存和数据缓存的命中率，降低Cache Miss。这和磁盘数据库的优化重点是不一样的，在磁盘数据库里，我们重点考虑如何在磁盘和内存缓冲区之间快速有效地交换数据。在内存数据库中，缓冲区管理器(Buffer Manager)不复存在。

对于一个数据库表(Relational Table，关系表)，数据可以按照三种格式来进行存储，分别是行存储、列存储和PAX。行存储在一个数据页(Data Page)中，一行一行地存储数据，每一行的数据是紧密排列在一起的。面向OLTP应用的数据库一般采用行存储的存储格式，因为OLTP事务一般仅仅存取少量的数据行。列存储格式把关系表划分成各个列，每个列单独连续存放。在分析型应用中，列存储格式比行存储格式获得超过一个数量级的性能表现。在这类应用中，查询需要扫描大量的数据，但是仅仅存取少量的数据列。当数据采用列存储格式，我们可以采用一些压缩技术来对各列数据进行高效的压缩，减少空间占用。

PAX(Partition Attributes Across)存储格式，是行存储和列存储的混合格式。首先，数据库表被横向划分成若干页面，每个页面内部各个属性(列)的值被连续存放。PAX格式极大地提高了CPU Cache的命中率，在进行数据Cache存取时，最高可以降低75%的等待时间，因为进行查询处理时，只需扫描相关的数据列。相对于行存储，TPC-H查询的执行时间可以提高11%～48%。此外，由于数据是首先进行横向划分的，PAX支持高度的扩展能力。

由此可见，行存储格式有利于执行OLTP负载，列存储格式有利于执行分析型(OLAP)负载，PAX格式则兼顾了OLTP和OLAP负载的性能。此外，为了支持混合负载，人们把面向读操作优化(Read-Optimized，比如列存储)的存储结构和面向写操作优化(Write-Optimized，比如行存储)的存储结构结合起来。已有的数据按照列存储格式保存，支持快速存取和操作，称为主副本。新增或者修改的数据用行存储格式保存在缓冲区(Write-Optimized Buffer)中，称为Delta数据。Delta数据定期合并到列存储格式的主副本中。

2. **索引技术**

当我们从数据库中查询数据时，如果有索引的帮助对数据进行定位，我们就可以避免全表扫描(Full Table Scan)。面向内存数据库的索引技术可以分成如下几类，分别是Cache敏感的索引(Cache Conscious Index)、Cache不敏感的索引(Cache Oblivious Index)以及特殊索引，比如面向非易失性内存(Non-Volatile Memory)的索引等。

Cache敏感的索引根据内存层次(Memory Hierarchy①)的一些参数，比如内存级别的数量、每个内存层次的块大小、各个内存层次之间的相对读写速度(R/W Relative Speed)等，设计紧凑的数据结构，尽量减少Cache Miss。Cache不敏感的索引，则独立于这些内存层次的参数进行设计，无论在什么样的硬件平台上运行，都获得可以接受性能。

比如Rao和Ross设计的Cache敏感的B+-tree索引“Cache Sensitive B+-tree”，在索引树中，每个节点的子节点连续存放，仅仅保存第一个子节点的开始地址(见图3-8)。其他子节点可以通过偏移量进行定位。由于每个节点的所有子节点只需保存一个指针，Cache敏感B+-tree索引提高了CPU Cache的利用率。

```
图3 - 8 Cache敏感的B+ 树索引(示意图)
```

3. **查询优化**

对于内存数据库来讲，查询优化的重点是如何利用多核CPU以及众核GPU，实现关键的数据操作的快速处理，比如选择、投影、连接、聚集等。

Kim等充分利用CPU Cache、线程级并行(Thread-Level Parallelism)、数据级并行(Data-Level Parallelism)、内存的高带宽(High Memory Bandwidth)等硬件特点，设计了新的连接操作算法，包括Hash Join和Sort-Merge Join，以充分发挥多核CPU的性能。经过优化，Hash Join能够达到100M元组/秒(Tuples/Second)的性能，比其之前的最好结果快17倍。Sort Merge Join则达到47M～80M元组/秒的性能。Albutiu等对Sort Merge Join进行了重新设计，考虑到内存存取的非对称性(None Uniform Memory Access, NUMA)，使得多核CPU的每个CPU在进行排序操作时，尽量存取隶属于该CPU的局部内存(Local Memory)，而不是隶属于其他CPU的远程内存(Remote Memory)，于是获得更高的处理速度。

Kaldewey等把关系数据库的Join操作迁移到(Offloading)GPU上运行，他们设计的Hash Join算法，充分利用PCI-E带宽(6.3GB/s)把数据传输到GPU上，并且利用GPU的众核处理器，加速Join操作的性能，获得比多核CPU快3X～8X的性能提升。Karnagel等则把分组和聚集操作(Grouping/Aggregation)迁移到GPU上运行。他们利用启发式规则，在运行时(Execution Time)，选择代价最优的算法和参数，提高分组和聚集操作的速度。

4. **并发控制**

并发控制是在多个事务并发执行的情况下，保证数据库状态正确的技术手段。并发控制有两大类算法，分别是基于加锁的并发控制和基于多版本管理的并发控制。根据深入的剖析，人们发现加锁(以及给数据结构加闩锁)占事务处理30%的指令计数。当数据完全驻留内存，我们有多个CPU核心可用时，设计合理的并发控制方法，才能有效发挥多核处理的威力。

Larson等为内存数据库设计了两个基于多版本管理(Multi-Versioning)的并发控制方法，分别使用乐观的和悲观的策略来保证事务的原子性。通过多版本管理，事务管理器有效地隔离读事务(Read Transaction)和写事务(Write Transaction)，读事务读取它应该读取的数据的最近已经提交的(Committed)版本，写事务则对数据进行拷贝(Copy a new Version)，生成新版本进行操作和读事务没有读写冲突。他们通过实验发现数据库的大部分事务都是短事务，在事务对数据存取的竞争程度较低的情况下，加锁方式获得较好的性能，但是在大量事务竞争存取热点数据以及负载里包含大量的长事务的情况下，基于多版本管理的并发控制技术，能够获得更高的性能。需要注意的是，多版本管理需要付出版本管理的开销。

Jones等设计了两个轻量级的加锁(Light Weight Locking, LWL)并发控制方法。其中一个并发控制方法无须跟踪事务对数据的读/写操作是否冲突，而是在提交事务时判别读/写冲突，如果发现此类冲突，则需要回滚(Roll Back)某些事务。在这样的情况下，就会浪费一部分已经做的工作。这个方法称为Speculative LWL方法。这个方法在TPC-C测试基准上，获得2倍的事务吞吐量。

5. **恢复技术**

当发生掉电情况，内存里的数据即刻消失(除了非易失性内存)。对于内存数据库来讲，恢复模块是保证其可靠性的重要模块。恢复模块的目标，是在系统失败以后尽快地把内存数据库给恢复过来。为了能够进行数据库恢复，恢复模块在正常事务处理过程中需要做一些簿记工作(Book Keeping)，这些簿记工作不要过多地干扰正常的事务处理。

磁盘数据库的恢复技术已经很成熟和高效，整个恢复子系统包括写日志、写检查点、恢复三个子模块。人们并没有从零开始设计和实现内存数据库的恢复模块，而是借鉴了磁盘数据库恢复技术的思想。当然，为了提高内存数据库的恢复效率，人们提出了一些新的方法。

Lee等提出了并行恢复(Parallel Recovery)技术，加快内存数据库的恢复过程。他们的恢复技术基于差分日志技术(Differential Logging)。差分日志通过把数据修改之前的值和修改之后的值进行异或差分(XOR Differencing)操作求出来。差分日志符合结合律和交换律(Commutative/Associative)，因此，日志记录可以按照任意顺序进行排列，写入磁盘，加快事务的执行。进行系统恢复时，对每个数据页面(Page)，日志记录应用①的顺序也无须符合串行顺序(Serialized Order)，可以乱序应用，于是每个数据页面可以独立地、并行地进行恢复，加快恢复过程。

Malviya则舍弃了传统的物理日志(Physiological Logging，记录数据的改变)方法，提出轻量的命令日志(Command Logging)方法。在事务执行时，系统记录事务对应的SQL命令以及参数。对数据库进行恢复时，首先把数据库的上一次一致检查点(Consistent Checkpoint)装载进来，然后重放(Replaying，即重新执行)日志里的所有命令即可。由于数据全部在内存中，重放过程是很快的。在TPC-C测试基准上，命令日志方法获得比传统的物理日志方法高1.5X倍的吞吐量。

①日志本身记录了事务对数据的修改，恢复过程中，日志记录应用(Log Record Applying)的含义是把这些修改施加到老版本的数据上，把数据修正到最新的正确状态。

随着非易失性内存价格的下降，以及其读写性能的提高，人们在考虑如何在数据库的恢复模块里应用非易失性内存。Schwalb等提出了Hyrise-NV数据库，该数据库在非易失性内存里，利用多版本数据结构，直接修改数据和索引(Data & Index)。所有修改都是事务性的，符合事务的ACID约束。系统失败后，Hyrise-NV几乎可以瞬间恢复整个数据库系统，提供新的服务。它完成有10000000行记录的表格的恢复，仅仅需要100ms。

### 3.2.4 MPP 并行数据库

MPP是Massive Parallel Processing的缩写。MPP并行数据库利用专用的数据库集群(Dedicated Cluster)的多个节点的并行处理能力，提高数据库的查询处理性能。为了提高并行数据库的性能，有时候MPP数据库运行在专门的硬件上，比如厂家定制的数据库一体机(Database Machine)。MPP并行数据库一般用于运行分析型负载(Analytical Workload)。分析式查询一般需要对大量数据进行扫描(Scan)、连接(Join)、聚集(Aggregation)等操作。

在架构方面，MPP并行数据库具有三种架构模式，分别是共享内存(Shared Memory)架构、共享磁盘(Shared Disk)架构和无共享(Shared Nothing)架构(见图3-9)。

```
(a)Shared Memory                       (b)Shared Disk                   (c)Shared Nothing (Shared Everything)
图3 -9 MPP数据库架构
注：P表示CPU，Mem表示内存，圆柱体表示磁盘存储，虚线框表示处理单元(节点)，处理单元(节点)间的连线表示高速互联网络。
```

- **Shared Memory架构**：在单个主机上，通过多个处理器(或者多核技术)实现并行处理。多个处理器(核心)共享主机的内存以及所有外部存储设备。每个处理器就是一个处理单元或者一个节点(Node)，处理器之间通过高速通信网络(Interconnection Network)互联。共享内存的系统，通过增加CPU和内存，提高系统的处理能力，其扩展能力有限。
- **Shared Disk架构**：指的是并行数据库的各个处理单元(节点)拥有自己的CPU和内存，但是共享磁盘存储系统。各个节点的处理器之间没有任何直接的信息和数据的交换，多个节点和磁盘存储由高速通信网络连接，每个节点都可以读写全部的磁盘存储。典型的系统有Oracle的RAC集群数据库系统。当系统的负载增加时，我们可以通过增加节点来提高并行处理能力，但是由于所有节点共享外存系统，所以存储带宽有可能成为系统的瓶颈，扩展能力有限。
- **Shared Nothing架构**：各个处理单元(节点)拥有自己独立的CPU、内存和磁盘存储，不存在共享资源，多个节点一起构成整个并行数据库系统。每个处理器使用自己的资源，处理自己的数据，不存在内存和磁盘的争用。每个节点负责管理一部分数据，并执行协调者(Coordinator)发送过来的全局事务(Global Transaction)的子事务(Sub Transaction)的操作。典型的系统，如Oracle公司的Exadata数据库。无共享架构具有最好的扩展能力。当我们为系统增加节点时，不仅提高了处理能力，而且提高了存储能力和I/O总带宽。

为了实现数据的并行处理，需要对数据库数据进行划分，分布到各个节点。数据的划分称为分片(Partitioning)，分片主要有Range、Hash等方法。Range方法把数据库表的记录，按照某个字段的值所属的范围进行分片。比如，当按照日期进行数据分片时，我们可以把不同月份的数据划分到不同分片。Hash方法则对于数据库表中的记录，根据一个或几个属性列的取值，利用哈希(Hash)函数计算一个函数值，映射到具体的分区，完成数据分片。

在MPP并行数据库中，数据分布在各个节点上，查询处理和集中式数据库有所不同。我们通过实例，介绍简单的单表查询和多表连接查询。

这里的单表查询指的是对一个数据库表进行选择、分组和聚集的查询。比如查询：

```sql
Select year(sale_date), product_name, sum(price * count)
From sales
Where productid = '101'
Group By year(sale_date)
```

注：从销售表中，选择产品101的销售记录，按照年度进行分组，计算销售额的汇总。

在MPP数据库系统里，一般有一个或者若干节点，接收用户的查询请求，并且担任事务协调者(Coordinator)的身份。Coordinator接收到用户的查询请求以后，根据数据分布的元信息，把查询进行分析、改写和分解发送给相关的节点运行。比如对于上述单表上的选择、分组、聚集查询，需要把该查询分发给包含相关数据的节点。

这些节点并行处理自己管理的数据，对数据进行扫描(Scan)、过滤(Filter)，并且对这些数据进行本地分组(Group)和聚集(Aggregate)，然后把结果发送给Coordinator。Coordinator获得所有子事务的本地聚集结果以后，对结果进行合并，返回给用户即可。在查询处理过程中，各个节点无须进行数据交换。

对于多表连接查询，我们以两个表的Hash Join作为例子。比如查询：

```sql
Select customer.name, sum(price * count)
From customer, sales
Where customer.id = sales.cid
And year(sale_date) >= 2015 and year(sale_date) <= 2016
Group By customer.id, year(sale_date)
```

注：对销售表和客户表进行连接查询，选择2015年和2016年的销售记录，按照客户和年度进行分组，汇总销售额。

对于该查询，当查询优化器选择使用Hash Join执行查询时，它把各个节点上用户表的记录，按照customer.id进行Hash操作，分发到查询节点(比如R1, R2, R3)上，同时把各个节点上销售记录表的记录，按照sales.cid(cid是sales的外键，表示某个销售记录隶属于某个用户，也就是把产品销售给了某个用户)进行Hash操作，分发到各个查询节点(比如R1, R2, R3)上。

由于上述Hash操作的Hash函数是一样的，并且customer.id和sales.cid的语义都是指某个用户，那么符合连接条件的用户记录和销售记录，被Hash分配到同样的查询节点。这些查询节点对收到的用户记录和销售记录进行连接操作以及后续的分组和聚集操作，然后把局部聚集结果返回Coordinator，由其进行结果合并返回用户。由此可见，在Hash Join的处理过程中，涉及了节点间大量的数据交换。

有时候，基于数据的分布特点和查询条件，优化器可以明智地对数据进行交换。比如对于上述实例，如果优化器了解到销售记录是按照sales.cid进行Hash划分的，那么执行查询时只需对customer表根据customer.id进行Hash操作，把客户表的记录按照客户ID，Hash分布到各个节点，就能保证每个节点对正确的数据进行连接操作。

如果销售表和客户表分别是按照sales.cid和customer.id实现进行Hash分布的，也就是两个将要连接的表格已经在连接的Key上进行Hash划分，那么进行查询处理时无须对数据再进行Hash分布，直接在每个节点上执行本地连接(Local Join)即可，这种情况称为数据已经预先并置(Co-Location)在一起了。更为一般地，数据并置指的是其中一个表的分片Key(Partitioning Key)是一个外键，比如上述实例中的sales.cid，另外一个表的分片Key包含了主键，比如上述实例中的customer.id，而且这两个表的分片数量(Partitions)是一样的。

在这种情况下，主外键连接，比如上述事例中的customer.id = sales.cid，可以在各个节点上通过执行本地连接实现。因为匹配(customer.id = sales.cid)的分区已经事先保存在各个节点上了，连接过程中无须通过网络交换数据。

在上述实例中，如果sales.cid上建立了聚簇索引(Clustered Index)，那么sales表的销售记录是按照cid进行排序保存的。这时，我们可以在customer表和sales表之间进行合并连接(Merge Join)。因为当我们扫描customer表和sales表时，每在customer表上扫描到一个客户，就可以在sales表上寻访到该客户的所有销售记录(因为sales的销售记录是按照cid进行排序的)。

1. **分布式事务处理**

在并行数据库中，数据分布在一系列节点上，事务处理的难度提高了。以转账事务为例，在集中式数据库中，只有一个物理节点，它了解事务的所有状态，通过加锁机制或者多版本并发控制机制，就能够保证事务的ACID特性，保证数据的一致性。但是在并行数据库里，各个节点之间在物理上相互独立，通过网络进行沟通和协调。相互独立的节点之间，无法准确地知道其他节点中的事务执行情况。如果想让分布式部署的多台机器中的数据保持一致性，就要保证在所有节点的数据写操作，要么全部执行，要么全部不执行。但是，一台机器在执行本地事务时，无法知道其他机器中的本地事务的执行结果，因此，这个节点也就不知道本次事务到底应该提交(Commit)还是回滚(Rollback)。常规的解决办法就是引入一个事务协调者(也称事务管理器)来统一调度各个节点的执行。

涉及多个节点的事务称为分布式事务。对涉及多个节点(数据)的分布式事务进行协调的协议有两阶段提交协议(2 Phase Commit)、三阶段提交协议(3 Phase Commit)等，下面对这些协议进行简单介绍并分析其优点和缺点。

2. **两阶段提交协议**

在并行数据库里，当一个事务涉及多个节点上数据的修改，其中一个节点被指定为事务协调者，其他节点则称为事务参与者。

两阶段提交协议的核心思想为，参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况，决定各参与者是提交操作还是中止操作。

两阶段提交协议包括两个阶段，分别是准备阶段(或投票阶段)和提交阶段(或执行阶段)。

(1) 准备阶段。事务协调者给每个参与者发送Prepare消息，每个参与者要么直接返回失败(如权限验证不通过)，要么在本地执行事务，写本地的Redo和Undo日志，但不提交，处于一种万事俱备，只欠东风的状态。可以将准备阶段划分成三个步骤：
- 协调者向所有参与者询问是否可以执行提交操作，并等待各参与者的响应。
- 参与者执行事务操作，并将Undo信息和Redo信息写入日志。
- 各参与者响应协调者发起的询问。如果参与者的事务操作实际执行成功，则返回一个“同意”消息给协调者；如果参与者的事务操作执行失败，则返回一个“中止”消息给协调者。

(2) 提交阶段。如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息，否则，协调者给所有参与者发送提交(Commit)消息。参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的资源(锁资源等)。

图3-10(a)和图3-10(b)分别展示了所有参与者的响应消息都为“同意”时的提交过程或者有参与者响应消息为“中止”时的回滚过程。

```
(a)                                                                  (b)
图3-10 两阶段提交协议
```

两阶段提交协议，看起来好像能够提供事务的ACID特性。两阶段提交协议具有内在的局限性：
- 同步阻塞。执行过程中，所有参与者都是事务阻塞的。也就是说，当参与者占有资源时，其他第三方程序访问这些资源将处于阻塞等待状态。
- 单点故障。协调者地位非常重要，一旦协调者发生故障，参与者会一直阻塞下去，尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定资源的状态中，将无法继续完成事务操作。虽然我们可以在协调者宕机后重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题。
- 数据不一致。在两阶段提交的第二个阶段中，当协调者向参与者发送Commit消息之后发生了局部网络异常，或者在发送Commit消息过程中协调者发生了故障，将导致只有一部分参与者接收到Commit消息。这部分参与者接到Commit消息之后，就会执行Commit操作，但是其他未接到Commit消息的机器则无法执行事务提交，在这种情况下，整个系统便出现了数据不一致的状况。
- 两阶段提交协议有一个无法解决的问题：协调者在发出Commit消息之后宕机，而且唯一接收到这条消息的参与者同时也宕机了，那么即使通过选举协议产生了新的协调者，这个事务的状态也是不确定的，没人知道事务是否已经被提交。

3. **三阶段提交协议**

由于两阶段提交协议存在同步阻塞、单点故障、数据不一致等问题，研究人员在两阶段提交协议的基础上提出了三阶段提交协议。三阶段提交协议是两阶段提交协议的改进版(见图3-11)。

```
图3- 11 三阶段提交协议
```

三阶段提交协议有两个重要改动：
- 引入超时机制，三阶段提交协议同时在协调者和参与者中都引入超时机制。
- 三阶段提交协议把两阶段提交协议的准备阶段一分为二，三阶段提交协议总共有三个阶段，分别是CanCommit, PreCommit, DoCommit三个阶段。

当事务处理进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求。协调者产生PreCommit消息的前提条件，是它在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。一旦参与者收到了PreCommit，意味着其他参与者都同意对数据进行修改。用一句话来概括，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到Commit或者Abort消息，但是它有理由相信，成功提交的概率是较大的。参与者无法及时收到来自协调者的信息，它会默认执行Commit。

相对于两阶段提交协议，三阶段提交协议解决了单点故障问题、减少了堵塞（因为一旦参与者无法及时收到来自协调者的信息，它会默认执行Commit，而不会一直持有事务资源并处于阻塞状态），但这种机制还是会导致数据一致性问题。比如，由于网络原因，协调者发送的Abort消息没有及时被参与者接收到，那么参与者在等待超时之后执行了Commit操作，这样就和其他接到Abort消息并执行回滚的参与者之间存在数据不一致的情况。

由此可见，无论是两阶段提交协议，还是三阶段提交协议，都无法彻底解决分布式系统的一致性问题。Google公司的Chubby分布式锁服务系统的作者Mike Burrows说过，“There is only one consensus protocol, and that's Paxos—all other approaches are just broken versions of Paxos.”。他的意思是，只有一种一致性算法即Paxos，其他算法都是Paxos算法的不完整版。由此可见，Paxos算法才是最终的解决方案。

4. **Paxos算法**

在Paxos算法中，总共有四种角色，分别是提议者(Proposer)、决策者(Acceptor)、产生议题者(Client)、最终决策学习者(Learner)。Proposer可以提出(propose)提案，Acceptor可以接受(accept)提案。在分布式系统中，达到数据的一致性，就是对某个数据的取值达成一致，比如对某个账户应该取什么值达成一致，也就是各个节点达成共识，或者说，Proposer、Acceptor、Learner等都认为同一个value被选定(chosen)。

Proposer、Acceptor、Learner分别在什么情况下才能认为某个value被选定呢？为了对某个value的取值达成共识，保证数据的一致性，Paxos算法在执行过程中必须符合一系列约束条件：
- Proposer：只要Proposer发的提案被Acceptor接受（需要半数以上的Acceptor同意才行），Proposer就认为该提案里的value被选定了。
- Acceptor：只要Acceptor接受了某个提案，Acceptor就认为该提案里的value被选定了。
- Learner：Acceptor告诉Learner哪个value被选定，Learner就认为那个value被选定。Learner的地位具有某种从属性。

Paxos算法的执行过程分为两个阶段。

(1) 第一阶段：
- ① Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。
- ② 如果一个Acceptor收到一个编号为N的Prepare消息，而且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。

(2) 第二阶段：
- ① 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N, V]提案的Accept消息给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer自己决定。
- ② 如果Acceptor收到一个针对编号为N的提案的Accept消息，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。

Paxos算法执行过程见图3-12。

```
	Proposer(N,V)	Acceptor(ResN,AcceptN,AcceptV)

第一阶段	Prepare(N),N递增，不重复
Prepare请求	如果N<=ResN，不响应或者响应error  如果N>ResN，则(a)令ResN=N，(b)
响应(Pok,AcceptN,AcceptV)或者 (Pok,Null,Null)

第二阶段	如果收到超过半数的Pok，发出Accept(N,V) 如果响应中有提案，则V=响应中最大的Ac- ceptN对应的AcceptV；如果响应都是(Pok, Null,Null)，那么V=Proposer自己定义的值
如果Pok未超过半数，重新获取N，发起 Prepare请求(回到第一阶段)	
		如果N>ResN，接受提案，令AcceptN= N，AcceptV=V，回复<Aok>
如果N<=ResN，不接受。不回复或者回复error
	如果<Aok>数过半，确定V被选定
如果<Aok>数不过半，重新发起Prepare 请求	
图3-12 Paxos算法执行过程示意图
```

Paxos算法是如何在分布式系统中保证数据一致性的，其证明请参考相关资料。①

① http://www.ux.uis.no/~meling/papers/2013-paxostutorial-opodis.pdf

## 3.3 结构化数据分析工具介绍  

结构化数据分析工具分为两大类，分别是传统的MPP数据库和列存储数据库，以及基于Hadoop的结构化数据分析工具，即SQL on Hadoop系统。前者包括MPP数据库比如Teradata、列存储数据库比如Vectorwise，后者包括Hive、SparkSQL、Impala、Presto等系统。

SQL on Hadoop系统依赖于HDFS(Hadoop Distributed File System)作为数据存储层。HDFS具有高度的扩展性和容错性（通过数据的多副本存放），于是SQL on Hadoop系统在存储层面支持超大规模数据的处理。在大数据时代，如何查询和分析TB级别甚至PB级别的数据是数据分析工程师不可回避的问题，SQL on Hadoop系统成为一个重要的工具。把SQL移植到Hadoop平台上的原因，是利用Hadoop的扩展能力管理大规模的数据，让用户能够使用他们熟悉的SQL语言对数据进行查询和分析。

此外，对于传统的MPP并行数据库来讲，它的容错保证需要依赖各个厂家专有的技术，甚至需要添加特殊的硬件。MPP并行数据库需要运行在特定的硬件平台上，各个厂家的软件也不是免费的。SQL on Hadoop系统则可以运行在廉价服务器构成的集群上，软件几乎是免费的。对MPP并行数据库的管理，需要管理员具有该数据库的管理经验。随着Hadoop技术的流行，人们可以通过各种途径获得管理Hadoop平台的经验，管理成本相对降低。

### 3.3.1 MPP(Shared Nothing)数据库、基于列存储的关系数据库

1. **Teradata**

Teradata数据库是面向大型数据仓库应用的基于Shared Nothing架构的并行数据库系统。Teradata系统由三部分构成，分别是处理节点(Node)、实现节点间通信的高速互联网络(Interconnection)、数据存储介质(一般是磁盘阵列)。图3-13展示了由4个节点构成的Teradata数据库系统。每个节点自底向上，包括操作系统软件(OS)、Teradata并行数据库扩展(PDE)和其他相关模块，包括PE和AMP。

(1) OS与PDE。Teradata并行数据库扩展(Parallel Database Extensions, PDE)，是直接架构在操作系统之上的一个接口层，用于为Teradata提供并行环境，并保证这个并行环境的可运行性和可靠性。PDE的主要功能是管理和运行虚拟处理器、进行Teradata并行任务调度、进行操作系统内核和Teradata数据库的运行时故障处理等。

(2) AMP。存取模块处理器(Access Module Processor, AMP)，是Teradata数据库的关键进程之一，用于处理所有与数据有关的文件系统的操作任务。一般情况下，一个节点上有多个AMP在工作，每个AMP分别负责文件系统上不同的数据的存取操作。

(3) PE。解析引擎(Parsing Engine, PE)，用于实现客户端(通常是使用Teradata数据库的应用程序的SQL请求)和存取模块处理器(AMP)之间的通信和交互。主要的功能包括会话控制(Session Control)、SQL语句的解析、优化、查询步骤的生成和分发，并行查询处理和返回查询结果。一个节点上通常只有一两个PE在工作。

(4) BYNET。在Teradata MPP系统中，各个节点间(即各个AMP之间)的内部高速互联是通过BYNET实现的，它由一组硬件和运行在这组硬件上的处理通信任务的软件进程构成。BYNET用于节点之间的双向广播(Bidirectional Broadcast)、多路广播(Multicast)和点对点(Point-to-Point)通信。

除了Teradata数据库，采用Shared Nothing架构模式的并行数据库系统还有Exadata数据库(来自Oracle)、XtremeData数据库等。

2. **SAP HANA**

SAP HANA是支持OLTP和OLAP混合负载的内存数据库系统。由于采用内存计算技术，HANA能够获得极快的事务响应时间和极高的事务吞吐能力。

HANA采用列存储技术进行数据的组织，同时对列存储数据进行有效的压缩。对数据进行压缩可以有效减少数据占用的空间。在进行查询处理时，即便考虑到解压缩的代价也是值得的，因为CPU比内存总线(Memory Bus)的速度要快很多，所以CPU可以及时地对数据进行解压缩。HANA支持数据的更新功能，其更新功能是通过添加数据实现的。删除数据时，旧的数据(记录)没有被覆盖掉，而是在表格的末尾添加(Append)。当一个查询执行时，它可以看到一个记录的若干版本，选取最近的提交版本(Committed Version)返回即可。

通过利用大内存以及列存储和数据压缩技术，HANA把整个数据集保存在内存中，实现了实时的查询处理。利用HANA的快速查询处理能力，人们可以获得秒级的响应时间(小于10秒)。性能评测结果显示，在相同的硬件环境和数据规模下，HANA往往能够获得超过Oracle(磁盘数据库)一个数量级左右的查询性能提升。①运行在HANA之上的报表系统，几乎无须用户等待，返回各种统计结果，给用户更好的体验。

HANA数据库产品是一个完备的工具套件，包括内存数据库服务器(MMDB Server)、建模工具(Studio)以及客户端工具(JDBC、ODBC接口等)。在一个数据库服务器上，HANA支持文本(Text)、空间数据(Spatial)、图(Graph)、流数据(Streaming)以及时间序列(Time Series)数据的处理能力，并且提供实时事务响应时间。通过集成R软件包，HANA方便地在数据上运行数据挖掘和机器学习算法，提供预测性分析能力(Predictive Analysis)。

① http://sapinsider.wispubs.com/Assets/Blogs/2013/September/Real-HANA-Perfornance-Test-Benchmarks.

3. **MonetDB/Vectorwise/VectorH**

MonetDB是荷兰研究机构CWI(Centrum Wiskunde & Informatica)于2003年开始研发的基于列存储的面向分析型应用的内存数据库系统。2008年MonetDB开发者基于MonetDB创立了Vectorwise公司，实现MonetDB的商业化。经过两年的研发，产品趋于成熟，并且性能优越，Vectorwise被Actian公司收购。

MonetDB和Vectorwise的主要区别在于其查询处理模式，MonetDB的查询处理模式是Column at a time，即一次处理一个数据列，Vectorwise则是Vector at a time，即一次处理一个向量（后面详细叙述）。一次处理一个数据列，需要对中间结果进行物化，交给后续的查询处理步骤继续处理。一次处理一个向量，则有利于把若干处理步骤整合起来，数据尽量地驻留在CPU Cache中，提高处理的效率。

为了尽快实现市场化，Vectorwise把MonetDB的X100存储引擎、查询引擎(Vectorized Query Engine)和Ingres数据库的上层模块结合起来。Vectorwise通过基于位置的Delta树(Positional Delta Tree, PDT)，维护数据的更新，支持数据的更新功能。进行查询处理时，首先在只读的主数据集上提取相关数据，然后根据PDT进行结果修正。图3-14展示了Vectorwise的查询执行过程涉及的PDT数据结构、缓冲区管理以及数据的存储等。

```
图3- 14 Vectorwise 系统架构
```

数据以列存储格式进行存放，并且保存在内存中。这时候CPU对内存的存取成为性能瓶颈(Bottlenecks)。Vectorwise针对这个瓶颈进行了一系列的优化，其中最重要的优化是向量化的查询处理模式(Vectorized Query Processing)。

向量化的查询处理模式是每个属性列的每100～1000个值构成一个向量，执行查询时，一系列的向量以流水线的方式，流过对该属性列进行操作的一系列操作符，由其处理，直到最后完成查询。换句话说，对某个属性列进行操作的一系列操作符，一次处理一个向量而不是整个列。这种查询处理模式可以充分利用现代CPU的SIMD指令(Single Instruction Multiple Data，单指令多数据)，通过并行操作，快速完成数据的处理。同时，这种查询处理方式下已经进入CPU Cache的向量尽量长时间地驻留，以便对该向量的所有操作符（构成操作符流水线）得到执行。此外，相关的数据属性列没有被存取，需要从内存中调人CPU Cache的数据量减少了，于是CPU的数据Cache和指令Cache的命中率都得到了提高。

下面以一个简单查询实例，解释向量化执行模式的查询处理过程。查询：

```sql
Select Sum(price * quantity)
From sales
Where sales.date = '2011-02-02'
```

在向量化执行模式下，查询处理器按照向量方式扫描日期(date)字段，然后获得一系列符合查询条件sales.date = '2011-02-02'的记录的偏移量。查询处理器继而提取对应的单价(price)字段和数量(quantity)字段的向量，根据这些偏移量，对单价(price)字段取值和数量(quantity)字段取值进行乘法操作，计算并累加总和。

数据库的查询处理引擎的处理模式可以分成四类，分别是一次处理一个元组(Tuple-at-a-Time Processing)、一次处理一块(Block-Oriented Processing)、一次处理一列(Column-at-a-Time Processing)和向量化查询处理(Vectorized Query Processing)。下面对这四种查询处理模式进行总结和比较。

(1) 一次处理一个元组。

这种查询处理模式最早出现在Volcano数据库系统中，因此也称为Volcano风格的(Volcano-Style)查询处理模式。该模式是在RDBMS系统中使用最广泛的查询处理模式，包括MySQL、SQLite、PostgreSQL、Oracle等系统都使用这种查询处理模式。

客户端提交的SQL查询，由查询解析器和优化器进行翻译，转换成查询执行计划(Execution Plan)，交给查询执行引擎。查询执行计划的每个操作符(Physical Operator)有三个接口，分别是open()、next()、close()。open方法初始化资源包括初始化内存数据结构或者打开文件，close方法则释放资源。

整个执行计划的执行是由根节点的next方法拉动的，父节点的next方法，从子节点提取一个元组，这个元组可能是由子节点调用其子节点的next方法获得的，直到对原始数据库表的扫描节点(Scan操作符)。每个操作符的next方法递归地调用子节点的next方法，最后输出一个元组，这就是Tuple-at-a-Time Processing查询处理模式的由来。比如，图3-15展示了这样的一个查询执行计划，该查询扫描两个表，然后进行Hash连接，最后进行分组聚集。

Tuple-at-a-Time Processing查询处理模式，一般使用行存储格式。查询执行引擎装载了不必要的字段信息，对于仅存取少量字段的分析型应用来讲，导致CPU Cache命中率较低。另外，整个查询语法树上的各个物理操作符交替执行，一会儿执行Scan，一会儿执行Join。这些指令有可能无法全部放入CPU的一级(Layer One，L1)指令Cache，导致频繁的Cache Miss，指令Cache的命中率不高。

```
图3-15 一次处理一个元组
```

(2) 一次处理一块。

Block-Oriented Processing查询处理模式是对Tuple-at-a-Time Processing查询处理模式的扩展。对next函数的调用，将返回一批元组(100～1000个)而不是一个元组。这种处理模式减少了函数调用的开销，获得比Tuple-at-a-Time Processing处理模式更高的CPU数据Cache和指令Cache的命中率，但是在分析型应用中，存取每条记录不必要的字段，仍然导致频繁的CPU数据Cache Miss。

(3) 一次处理一列。

Column-at-a-Time Processing查询处理模式是最初实现的查询计划列式执行(Columnar Execution)模式。该查询处理模式使用列存储格式，每个数据列单独进行存放(紧密排列的数组，Dense-Packed Array)和后续处理。每个操作符的next方法，处理整张表格的某一列，并且把中间结果保存起来。为了支持这种处理模式，需要定义和实现若干操作原语(Primitive)。这些操作原语在一个紧凑的循环中(Tight Loop)对数组的各个元素进行处理。一般来讲，对这些循环处理进行翻译后的指令，容易装入CPU的L1级指令Cache，提高指令Cache的命中率。此外，操作原语对数组进行操作，生成一个新的数组，具有良好的数据Cache的局部性，即要操作的数据可能已经在数据Cache中。

智能编译器对SQL查询进行翻译时，能够利用自动向量化技术(Auto-Vectorization)，通过SIMD(Single Instruction Multiple Data)指令实现数据处理。通过CPU数据级的并行处理能力，提高数据处理效率。

但是Column-at-a-Time Processing查询处理模式也有局限性，因为它对一个表格(a Whole Table)的整个数据列进行操作，于是中间结果集可能很大，需要物化(Materialized)到内存或者磁盘中，于是造成CPU的数据Cache Miss以及I/O开销。

(4) 向量化查询处理。

Vectorized Query Processing查询处理模式前文已经简单介绍过，它是最先进的列式执行模式，解决了Column-at-a-Time Processing查询处理模式的缺点。这种处理模式建立在Column-at-a-Time Processing模式之上，只不过在每个next函数调用中，操作符处理的是某列数据的一个部分(Chunk)，称为一个向量，而不是表格的(某个)整个列。向量的大小设定，考虑了CPU Cache的大小，很容易装载到CPU Cache中，提高Cache命中率。

正如上文介绍过的，这种处理模式可以充分利用CPU的SIMD指令进行数据处理，CPU的数据Cache和指令Cache的命中率都得到了提高。

此外，由于各个操作原语的原理很简单，我们可以使用C语言或者汇编语言实现这些原语，使得编译器能够产生高效的执行代码。

4. **Vectorwise MPP原型与VectorH**

在Vectorwise数据库基础上，Actian公司研发了MPP并行数据库原型，这是一个采用Shared Nothing架构的并行数据库系统。在这个原型中，除了各个节点的Vectorwise进程外，为了把各个Vectorwise进程连接起来，增加了基于MPI(Message Passing Interface)通信机制的查询处理模块、在各个节点间进行数据交换的Exchange操作符以及查询改写模块(Parallel Rewriter)，如图3-16所示。

```
Client App/BI Tools
SQLQuery result

SQL Parser
Parsed tree
Optimizer
Query plan
Cross Compiler

Distributed rewriter上
Annotated query tree
Builder
Operator tree
Execution Engine
Data request data
Buffer Manager I/O
HDFS

MPI
Annotated query tree

MPI
Partial resultset

rewriter
Annotated query tree
Builder
Partial operator tree
Execution Engine
Data request data Buffer Manager
I/O

HDFS

图3 - 16 VectorH 系统架构
```

基于这个MPP数据库原型，Actian公司继续把Vectorwise迁移到Hadoop平台上，利用HDFS作为存储层，研发了SQL on Hadoop系统VectorH。VectorH扩展了Vectorwise MPP并行数据库原型系统，利用Hadoop平台的YARN资源管理器，实现工作负载和资源调度。

依赖于HDFS，VectorH自然地获得了存储层的扩展性和容错性能。此外，(1) VectorH对HDFS的块复制策略(HDFS Replication Policy)进行了干预，控制数据块如何在各个节点间进行复制和放置，目的是优化读操作的局部性(Read Locality)。(2)虽然HDFS只能进行数据添加(Append)，但是VectorH通过基于位置的Delta树(Positional Delta Trees, PDT)实现了数据更新(Update)功能。PDT是一个对数据更新进行追踪的差分数据结构(Differential Update Structure)，它支持快速的查询功能。

基于列存储、面向分析型应用的数据库还有Netezza、Vertica、Sybase IQ、ParAccel等。

### 3.3.2 SQL on Hadoop系统

Hive on MapReduce和Hive on Tez是Hadoop平台上的结构化大数据分析工具，SparkSQL则是Spark平台上的结构化大数据分析工具。对这两个工具的具体介绍，请分别参考关于Hadoop平台和Spark平台的相关章节。在这里，我们介绍Impala和Presto两个SQL on Hadoop系统。

1. **Impala**

Impala是Cloudera公司开发的大数据实时查询系统。它提供SQL查询接口，能够查询存储在Hadoop的HDFS和HBase中的PB级大数据。Hadoop平台上的数据仓库系统Hive也提供SQL查询支持(通过类似于SQL的HQL语言)，但是由于Hive查询需要转换成MapReduce Job，由MapReduce引擎执行，是一个批处理过程，不能满足交互式查询的性能要求。Impala则试图填补这个空白，提供快速的数据查询响应时间，在Hadoop上实现交互式查询能力。

Impala的实现借鉴了Google公司的Dremel系统。Dremel是Google公司开发的交互式数据分析系统，它构建于Google的GFS(Google File System)文件系统之上，支撑了数据分析服务Big Query等诸多服务。Dremel的主要技术特色包括：
- 实现了嵌套的列存储数据结构。
- 使用多层查询树，使得任务可以在上千节点上并行执行，不断对结果进行聚集。其中，Dremel的多层查询树的根节点负责接收查询，并将查询分发到下一层节点……底层节点负责具体的数据读取和查询执行，然后将结果返回上层节点。中间层次可能有多层，逐层进行数据汇总，最后由根节点把结果返回给客户端，如图3-17的左侧子图所示。每个节点内部有一棵查询执行树(Query Execution Tree)，是整个查询执行计划的一部分，如图3-17的右侧子图所示。

```
Client                                 Query Execution Tree

Root server

Intermediate
servers

Leaf servers
(with local storage)
Storage Layer(e.g.GFS)
图3- 17 Dremel的多层查询树
```

Impala的系统架构如图3-18所示。Impala支持主流的SQL语言功能，包括Select、Insert、Join等操作。表格的元数据信息存储在Hive的MetaStore中。State Store是Impala的一个服务，用来监控集群中各个节点的健康状况，提供节点注册、错误检测等功能。Impala在每个节点运行了一个后台服务进程Impalad，用来响应外部请求，并完成实际的查询处理。

```
Common Hive SQL&Interface                               Unified meta data

图3-18 Impala 系统架构
```

Impalad包含Query Planner、Query Coordinator和Query Exec Engine三个模块。Query Planner接收来自客户端的SQL查询请求，然后将其查询转换为许多子查询。Query Coordinator将这些子查询分发到各个节点上，由各个节点上的Query Exec Engine负责子查询的执行，最后返回子查询的结果。这些中间结果集不写入磁盘，直接在不同的操作符和节点之间通过流水线(Pipeline)方式进行传递。

Impala的查询效率相比Hive有数量级的提升。之所以能有这么好的性能，主要有如下几方面的原因：
- Impala不需要把中间结果写入磁盘，节省了大量的I/O开销。
- Impala像Dremel一样，借鉴了MPP并行数据库查询处理技术的思想，抛弃MapReduce计算模型，对查询进行深入的优化，省略不必要的Shuffle、Sort等操作。
- MapReduce启动任务(task)的速度是很慢的(默认每个心跳间隔是3秒钟)，Impala直接通过服务进程来进行任务调度，节省了MapReduce任务(task)启动的开销，查询处理速度快了很多。
- Impala使用LLVM(Low Level Virtual Machine)来编译运行时代码，提高了查询计划的执行效率。
- Impala使用C++语言实现，针对硬件进行了针对性的优化，包括使用SSE指令集提高数据处理效率等。
- Impala基于数据的局部性(Data Locality)进行I/O调度，尽量把数据和计算分配在同一个节点上，减少了网络开销。此外，Impala使用Parquet列存储格式，提高分析负载的执行效率。

虽然Impala的查询处理性能比Hive(Hive on MapReduce)要高很多，但是Impala并不能取代已有的MapReduce数据处理系统，而是作为MapReduce数据处理系统的一个补充。Impala适用于处理数据规模适中的SQL分析查询，对于大量数据的批量处理任务，MapReduce以及Hive on MapReduce仍然是可信赖的选择。

2. **Presto**

Presto是由Facebook公司贡献出来的开源SQL on Hadoop项目(2012年开发，2013年开源)。它的目标和Impala是类似的，即在Hadoop平台上提供对结构化数据的交互式(Interactive)查询能力。Teradata公司除了维护其Teradata数据库的市场领导地位之外，也在Hadoop平台以及SQL on Hadoop工具等方面大量投入。自从该公司收购了Hadapt以后，它安排其中的部分工程师跟进Presto项目，并对该软件做出贡献。

由于提供了交互式查询能力，Presto在查询处理层不提供容错保证。当使用Presto执行查询失败时，用户重新提交该查询即可。如果数据存放在HDFS中，我们可以在数据上同时运行Presto和Hive。Presto主要用于执行交互式查询(响应时间在秒级，即小于10秒)，这类查询扫描的数据量一般较少，换句话说，这类查询只是扫描大数据集的一小部分。Hive则主要用于执行长查询(Long Query)，这类查询扫描的数据量较大，比如PB级别，执行时间较长。

对于交互式查询，Presto选择不予提供查询容错保证，降低软件的复杂度，提高执行效率。Hive on MapReduce在执行层面，通过中间结果存盘，提供了容错保证，保证长查询可以最终执行完毕，无论查询执行过程中是否发生各种失败状况(比如节点失败)。

Presto的主要特点包括：
- 数据通过流水线方式，从一个操作符传送到后续操作符，在没有必要的情况下，中间结果不用写磁盘。
- Presto的查询处理模式是向量化(Vectorized)的执行模式。
- Presto的执行计划(以及操作符)，被动态编译成字节码(Byte Code，一种通用的机器指令)，然后执行。
- Presto虽然用Java语言编写，但是它不使用Java的内存管理接口，而是直接进行内存管理(Direct Memory Management)，因此它的内存管理是高效的。

Presto的系统架构如图3-19所示。Presto采用Master-Slave架构，它由一个Coordinator节点、多个Worker节点组成。Coordinator节点中通常内嵌一个Discovery Server。Coordinator负责解析SQL语句，生成执行计划，分发执行任务给Worker节点执行。Worker节点负责实际执行查询任务。Worker节点启动后，向Discovery Server服务注册，Coordinator从Discovery Server获得可以正常工作的Worker节点列表。

```
图3 - 19 Presto系统架构
```

Presto通过插件方式(Connector，插件，如图3-20(a)所示)，支持不同的存储引擎。比如，Presto通过Hive Connector可以存取Hive的数据。Presto通过Hive Connector的元信息接口(Metadata API)，存取Hive MetaStore的元信息。调度器(Scheduler)通过Hive Connector的数据位置接口(Data Location API)，了解数据的位置信息。每个Worker节点通过Hive Connector的数据流接口(Data Steam API)，与HDFS交互读取数据。如图3-20(b)所示。

```
(a) 系统架构里的存储引擎                       (b) 查询处理流程中的存储引擎
图3-20 Presto
```

通过插件方式支持不同的后端存储引擎 (Backend Storage Engine)Presto处理的最小数据单元是一个Page对象，Page对象的内部结构如图3-21所示。一个Page对象包含多个Block对象，每个Block对象是一个字节数组，存储一个字段的若干Value（即对应若干行）。从逻辑上看，多个Block的相应的偏移量上的值构成表格的一行数据。

```
图3-21 Presto的数据模型
```

Presto服务器对其接收到的SQL查询，大致经过如下几个步骤进行解析和执行（见图3-22）。首先Parser对SQL语句进行词法和语法解析，生成statement结构。然后Analyzer利用元信息对语法树进行检查，生成analysis结构。Planner利用Optimizer的帮助进行优化，生成逻辑计划plan。Scheduler根据数据的位置信息，把逻辑计划划分成物理计划片段sub plan，分发给若干Worker节点，由其执行。

```
图3-22 SQL 语句的解析和执行
```

下面通过一个SQL查询实例，解释如何由逻辑计划转换成物理计划，以及物理计划片段如何在Worker节点上调度。比如查询：

```sql
Select c1.rank, count(*)
From city c1 join city c2
On c1.id = c2.id
Where c1.id > 10
Group By c1.rank
Limit 10
```

由该SQL查询语句生成的逻辑执行计划(Logical Execution Plan)如图3-23(a)所示。逻辑执行计划图中的虚线是Presto对逻辑执行计划进行切分的切分点。这些切分点把逻辑计划划分成四个子计划(sub plan)，每个sub plan交给一个或者多个Worker节点运行。先导和后续sub plan之间需要进行数据交换，如图3-23(b)所示。

```
(a) 查询执行计划                     (b) 查询执行计划的子计划
图3-23 一个SQL的逻辑执行计划
```

该SQL语句的各个sub plan的实际执行过程，请参考图3-24，具体说明如下：

(1) Coordinator通过HTTP协议，调用Worker节点的任务(task)接口，将执行计划分配给Worker节点（图3-24中虚线箭头）。
(2) 执行sub plan 1的每个Worker节点，读取一个Split的数据并进行过滤后，将数据分发给每个sub plan 0节点，进行连接(Join)操作和局部聚集(Partial Aggregation)操作。
(3) 执行sub plan 0的每个Worker节点，计算完成后，按Group By Key的Hash值，将数据分发到不同的sub plan 2节点（Group By是分组操作，Group By Key是分组操作的Key）。
(4) 执行sub plan 2的每个Worker节点，进行全局聚集，计算完成后，将数据分发到执行sub plan 3的Worker节点，执行limit操作。
(5) 执行sub plan 3的Worker节点计算完成后，通知Coordinator结束查询，并将数据发送给Coordinator，由其返回客户端程序。

```
图3 - 24 Sub Plan的分配、依赖关系和执行过程
```

Presto支持节点内部的流水线计算和节点间的流水线计算。图3-25(a)表示多个任务的执行流程，图3-25(b)表示每个任务的执行流程。

```
Pending split pool
Split       Split
Take one
Execute  for
1    second
Time is up

Done

Remove the split

Op1

0p2

Op3

Page=op1.getOutput()     Op2.addlnput(page)
Page =op2.getOutput()
Op3.addlnput(page)

(a) 任务间的调度流程                        (b) 任务内的各个Operator的执行流程
图3 - 25  Worker 节点内部的计算流程
```

在一个Worker节点内部，Worker节点将最细粒度的任务封装成一个Prioritized Split Runner对象，放入pending split pool优先级队列中。Worker节点启动一定数目的线程进行计算。每个空闲的线程，从队列中取出一个Prioritized Split Runner对象，然后执行。如果超过最大执行时间即1秒钟，判断任务是否已经完成，如果已经完成，就从split队列中删除该split，否则放回pending split pool队列，以便后续继续调度，执行未完成的工作。

每个任务的执行流程是，依次遍历各个操作符(Operator)，尝试从上一个操作符取得一个Page对象，如果该Page对象不为空，那么交给下一个操作符执行。

节点间的流水线计算，通过Exchange操作符(Operator)实现（见图3-26）。Exchange Operator为每一个Split启动一个HttpPageBufferClient对象，主动向上一个阶段（即sub plan）的Worker节点的sink操作符拉数据。数据的最小单位是一个Page对象，取到数据后放入Pages队列中。这个Pages队列，由本地的下一个阶段（即sub plan）的操作符进行消费。

```
图3 - 26  节点间的流水线计算(C:HttpPageBufferClient)
```

### 3.3.3 性能比较

我们从参考文献中选取了部分第三方性能评测结果呈现出来，让读者可以了解到主流的结构化数据分析系统的相对性能。需要注意的是，这些评测结果是在一定的软硬件条件和数据规模下，在某个时间获得的结果。考虑到各个开源社区对开源软件以及各个厂家对其产品的不断改进，这些系统的性能也在不断提高，其相对性能也会发生变化。

1. **AMP Lab Benchmark**

加州大学伯克利分校的AMP Lab于2014年初对Redshift、Hive on MapReduce (v0.12)、Shark(v0.8.1)、Impala(v1.2.3)、Hive on Tez(v0.2.0)等结构化数据分析系统进行了性能评测。其中，Redshift是一个来自Amazon的基于ParAccel数据仓库技术的MPP数据库。Shark为SparkSQL的前身，它是Spark平台上与Hive兼容的SQL查询引擎。

所使用的硬件平台是在Amazon云平台(Amazon Elastic Compute Cloud, EC2)上创建的虚拟集群。测试数据集包括三张数据库表。Ranking表保存各个页面和它们的Page Rank，UserVisits表保存对每个Web页面的用户存取日志，Documents表保存每个页面的非结构化的HTML页面内容。

所使用扫描查询(Scan Query)、聚集查询(Aggregation Query)、连接查询(Join Query)等不同查询负载，对目标系统进行了性能测试。其中Join Query查询对应的SQL语句如下：

```sql
-- 注：上述查询包含一个子查询。子查询对Ranking表和UserVisits表进行连接，然后以sourceIP作为分组条件，统计1980年1月1日以来到某个时间为止（用户提供该参数）的每个sourceIP的平均的PageRank和广告营收(adRevenue)的总和。外围查询在上述查询结果基础上，把广告营收最高的sourceIP选择出来。
```

Join查询在各个系统（配置）上的实验结果如表3-5和图3-27所示。当选择率比较低时，Spark(In-Memory，表示数据驻留在内存中)获得和Redshift及Impala(In-Memory)类似的响应时间，并且比其他系统（配置）的性能都要好。当选择率比较高时，Spark(In-Memory)比其他系统（配置）的性能都要好，除了Redshift。Redshift总是获得比Spark更高的性能。

从这个结果我们可以看出，Redshift的性能是最好的，Impala和Shark性能接近，两者的性能都比Hive高出很多。Hive(on MapReduce)的性能最差，Hive on Tez的性能相对于Hive有所改进。

**表3-5 各个系统（配置）执行Query 3（不同选择率）的响应时间的中位数**

| System(Setting) | Version | Query 3A (485312 rows, low selectivity) | Query 3B (53332015 rows, median selectivity) | Query 3C (533287121 rows, high selectivity) |
| --------------- | ------- | --------------------------------------- | -------------------------------------------- | ------------------------------------------- |
| Redshift(HDD)   | Current | 33.29                                   | 46.08                                        | 168.25                                      |
| Impala-Disk     | 1.2.3   | 108.68                                  | 129.815                                      | 431.26                                      |
| Impala-Mem      | 1.2.3   | 41.21                                   | 76.005                                       | 386.6                                       |
| Shark-Disk      | 0.8.1   | 111.7                                   | 135.6                                        | 382.6                                       |
| Shark-Mem       | 0.8.1   | 44.7                                    | 67.3                                         | 318                                         |
| Hive-YARN       | 0.12    | 561.14                                  | 717.56                                       | 2374.17                                     |
| Tez             | 0.2.0   | 323.06                                  | 402.33                                       | 1361.9                                      |

*这里的配置，指的是数据驻留在内存中还是磁盘中。

```
图3 - 27  各个系统（配置）执行Query 3 (不同选择率)的响应时间的中位数
```

需要指出的是，这几个软件，包括Impala和Hive以及Shark（现在是Spark SQL），都在不断改进之中，当它们使用了更多的优化技术以后，其性能的相对关系将发生变化。关于其他查询的实验结果以及最新的实验结果，读者可以参考AMP Lab网站。①

① https://amplab.cs.berkeley.edu/benchmark/

2. **Actian Benchmark**

Actian公司于2016年使用TPC-H数据集(Scale Factor为1000)，对VectorH(5.0)进行了性能评测，并且和Impala(2.3)、Hive(1.2.1)、HAWQ(1.3.1)以及SparkSQL(1.5.2)进行了对比。Actian公司使用的集群由10个节点组成，运行Hadoop 2.6.0，其中一个节点作为Name Node，其余9个节点用于进行SQL on Hadoop系统的性能试验。每个节点有2个Intel Xeon E5-2690v2 CPU(3GHz)，256GB内存，整个集群总共有20个CPU核心(40个超线程)。节点间通过10Gbit Ethernet网进行连接。每个节点有24个600GB的磁盘，其中一个安装操作系统，其余用于存储数据（使用HDFS分布式文件系统）。

Actian公司使用了Hive所有可用的优化技术，包括Tez执行引擎(Tez Execution Engine)、向量化执行模式(Vectorization)、谓词下推(Predicate Pushdown)、基于代价的优化器(Cost-Based Optimizer)。数据采用ORC格式（一种列存储格式）进行存储，采用snappy压缩技术进行压缩。

HAWQ是来自EMC Green plum公司的SQL on Hadoop系统。给HAWQ的每个节点设定了21GB的vmem参数，其中12GB作为work memory，6GB用于暂存查询语句(Query Statement)。每个节点开辟256MB的共享缓存(Shared Buffer)。Orders表和Lineltem表采用Parquet格式（另外一种列存储格式）进行存储，采用snappy压缩技术进行压缩。

在SparkSQL配置中，采用了snappy压缩方式的Parquet格式进行数据存储。试验结果如图3-28和表3-6所示。VectorH系统获得了秒级的响应时间，比HAWQ、SparkSQL、Impala、Hive等系统快得多，超过一个数量级（甚至达到2～3个数量级）。

```
How Many Times are VectorH Faster than These Systems?

图3-28 VectorH比各个系统快多少倍
```

**表3-6 TPC-H(Scale Factor 1000)数据集上各个查询的响应时间（秒）**

| Query | VectorH | HAWQ  | SparkSQL | Impala | Hive  |
| ----- | ------- | ----- | -------- | ------ | ----- |
| Q1    | 1.5     | 158.2 | 155.4    | 585.4  | 490.1 |
| Q2    | 1.14    | 21.46 | 74.98    | 81.81  | 63.57 |
| Q3    | 3.16    | 32.06 | 62.38    | 167.7  | 266.6 |
| Q4    | 0.17    | 38.21 | 68.27    | 163.18 | 59.08 |
| Q5    | 1.94    | 36.38 | 146.5    | 242.5  | DNF   |
| Q6    | 0.31    | 20.19 | 5.1      | 1.81   | 63.63 |
| Q7    | 2.75    | 44.74 | 180.2    | 369.0  | 721.8 |
| Q8    | 1.31    | 48.38 | 174.6    | 276.2  | 625.6 |
| Q9    | 11.11   | 766.4 | 264.0    | 1242.9 | 1077  |
| Q10   | 1.21    | 32.97 | 56.62 | 69.97 | 230.5 |
| Q11 | 1.69 | 12.48 | 30.28 | 35.04 | 246.1 |
| Q12 | 0.34 | 31.75 | 66.97 | 45.67 | 65.78 |
| Q13 | 3.66 | 27.97 | 47.65 | 180.8 | 140.7 |
| Q14 | 0.83 | 19.47 | 6.92 | 13.95 | 53.23 |
| Q15 | 1.63 | 31.58 | 11.16 | 15.19 | 556.5 |
| Q16 | 1.68 | 14.17 | 33.81 | 47.52 | 92.51 |
| Q17 | 1.24 | 173.2 | 244.9 | 581.53 | 711.7 |
| Q18 | 0.99 | 87.08 | 254.7 | 1234 | 454.5 |
| Q19 | 1.32 | 24.82 | 24.89 | 714.7 | 1010 |
| Q20 | 2.15 | 42.84 | 31.56 | 74.25 | 100.5 |
| Q21 | 1.48 | 84.7 | 1614 | 880.8 | 247.7 |
| Q22 | 2.84 | 29.44 | 91.18 | 34.81 | 81.11 |

3. **AtScale Benchmark**

2016年底，AtScale公司发布了针对主流的SQL on Hadoop结构化大数据分析平台的性能评测结果。所评测的系统包括SparkSQL(2.0.1)、Impala(2.6)、Hive/Tez(2.1，Tez版本为0.8.5)和Presto(0.152)。

AtScale公司进行测试的集群包括12个节点，其中1个节点为Master Node，1个节点作为Gateway Node，其余10个节点作为Data Node。每个节点有128GB内存，Data Node有32个CPU核心以及2个512MB的SSD（固态硬盘）。SparkSQL和Impala使用Parquet列存储格式，Hive和Presto使用ORC列存储格式。

AtScale公司采用SSB(Star Schema Benchmark)评测基准，对这些系统进行测试。在其中的一个测试中，LineOrders的记录数达到60亿条。实验结果如图3-29和表3-7所示。

```
Large Query Performance, Response Time(Sec)

Query ID
图3-29 采用SSB测试基准对四个系统进行评测的结果
```

**表3-7 采用SSB测试基准对四个系统进行评测的结果**

| Query | Query Execution Time(秒) |
| ----- | ------------------------ |
|       | Impala 2.6               |
| Q1.1  | 5.6                      |
| Q1.2  | 5.0                      |
| Q1.3  | 5.6                      |
| Q2.1  | 8.0                      |
| Q2.2  | 6.2                      |
| Q2.3  | 6.0                      |
| Q3.1  | 12.6                     |
| Q3.2  | 15.5                     |
| Q3.3  | 8.0                      |
| Q3.4  | 7.4                      |
| Q4.1  | 97.3                     |
| Q4.2  | 49.1                     |
| Q4.3  | 26.8                     |

新版本的Impala、SparkSQL和Hive都相对于其各自早先版本有了较大的性能提升。基于该评测结果，得出的结论是：

(1) 没有一个SQL on Hadoop在所有的查询上都获得最好的性能。SparkSQL和Impala比Hive要快得多。在多数查询上，Impala和SparkSQL的性能差异很小。Presto获得了和Hive相似的性能。
(2) 随着一个查询里Join操作数量的增加，查询处理时间也相应增加。当Join操作的数量从1增加到3时，Hive和Presto的查询响应时间的变化较大。Impala和SparkSQL则比Presto和Hive获得更好的性能。
(3) 随着查询选择率的提高，查询响应时间也相应增加。Hive和Presto对查询选择率的敏感度相对较弱，也就是随着选择率提高，响应时间的变化没有那么大。
(4) 在所有的系统上，在两张大表之间进行Join操作，查询的响应时间都会变得很慢。一般来讲，对两张大表进行Join操作，比如Customer表达到10亿条记录，是一个代价极大的操作。

## 3.4 思考题

(1) 数据仓库、星型模型。
(2) 联机分析处理的主要操作。
(3) 联机分析处理的三种实现技术。
(4) 列存储技术。
(5) 位图索引技术。
(6) 内存数据库技术。
(7) 并行数据库架构、数据分片、查询处理。
(8) Teradata数据库及其特点。
(9) SAP HANA数据库及其特点。
(10) MonetDB/Vectorwise/VectorH系统及其特点。
(11) Impala系统及其特点。
(12) Presto系统及其特点。
